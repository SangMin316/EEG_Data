{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ukeRIMnjOeQyZg--8BP7VlDityZ62pGR",
      "authorship_tag": "ABX9TyMUtdpMKyrPKU+Ea1ERqTUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangMin316/EEG_Data/blob/main/220929Data_Loader_concat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB6nI9xkSVFG",
        "outputId": "79b720b6-222c-4da7-b664-671ac238be22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mne"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pickle\n",
        "import mne\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Sleepedf_dataset(Dataset):\n",
        "    def __init__(self, files, seq_len):\n",
        "        self.files = files\n",
        "        self.sequence_length = seq_len\n",
        "        \n",
        "        # sample을 split해줬을 때 몇개로 split되는지 누적해서 저장, i번째 data를 찾을 때 data_adress 각 값이 기준이 됨\n",
        "        data_adress = [0]\n",
        "        ad = 0\n",
        "        for i in range(len(self.files)):\n",
        "            with open(file = files[i] , mode='rb') as f:\n",
        "                sample = pickle.load(f)\n",
        "            c,t = sample.shape\n",
        "            t = int(c*t*2/self.sequence_length) \n",
        "            ad += t\n",
        "          \n",
        "            data_adress.append(ad)\n",
        "        self.data_adress = data_adress\n",
        "\n",
        "\n",
        "    def preprocessing(self, data):\n",
        "        data = mne.filter.resample(data, up = 2.0) # upsampling to 200Hz  \n",
        "        return data\n",
        "\n",
        "\n",
        "    def split_data(self, data):\n",
        "        L = self.sequence_length\n",
        "        channels, length = data.shape\n",
        "        a = L*int(length/L)\n",
        "        \n",
        "        if length == a:\n",
        "            data = np.reshape(data,(int(length/L*channels),1,L))\n",
        "        \n",
        "        else: # data가 sequence_length로 나눠지지 않을때, 앞에 a부분만 취하겠다.\n",
        "            data = data[:,:a]\n",
        "            data = np.reshape(data,(int(a/L*channels),1,L))\n",
        "        data = np.squeeze(data,1)\n",
        "        return data\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        for i in range(len(self.data_adress)): \n",
        "            if index < self.data_adress[i]: # index 찾을 때 어떤 파일에서 찾아야 하는지 서치\n",
        "                break\n",
        "          \n",
        "        with open(file = self.files[i-1] , mode='rb') as f:\n",
        "            sample = pickle.load(f)\n",
        "            \n",
        "        sample = self.preprocessing(sample)\n",
        "        sample = self.split_data(sample)\n",
        "          \n",
        "        return sample[index - self.data_adress[i-1],:]\n",
        "          \n",
        "    def __len__(self):\n",
        "        return self.data_adress[-1]\n"
      ],
      "metadata": {
        "id": "H3L9w9iqTBOQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MASS_dataset(Dataset):\n",
        "    def __init__(self, files, seq_len):\n",
        "        self.files = files\n",
        "        self.sequence_length = seq_len\n",
        "        \n",
        "        # sample을 split해줬을 때 몇개로 split되는지 누적해서 저장, i번째 data를 찾을 때 data_adress 각 값이 기준이 됨\n",
        "        data_adress = [0]\n",
        "        ad = 0\n",
        "        for i in range(len(self.files)):\n",
        "            with open(file = files[i] , mode='rb') as f:\n",
        "                sample = pickle.load(f)\n",
        "            c,t = sample.shape\n",
        "            t = int(c*t*2/self.sequence_length)\n",
        "            ad += t\n",
        "          \n",
        "            data_adress.append(ad)\n",
        "        self.data_adress = data_adress\n",
        "\n",
        "    def preprocessing(self,data):\n",
        "        data = mne.filter.resample(data, down = 1.28) # downsampling to 200Hz  \n",
        "        return data\n",
        "\n",
        "    def split_data(self, data):\n",
        "        L = self.sequence_length\n",
        "        channels, length = data.shape\n",
        "        a = L*int(length/L)\n",
        "        \n",
        "        if length == a:\n",
        "            data = np.reshape(data,(int(length/L*channels),1,L))\n",
        "        \n",
        "        else:\n",
        "            data = data[:,:a]\n",
        "            data = np.reshape(data,(int(a/L*channels),1,L))\n",
        "        data = np.squeeze(data,1)\n",
        "        return data\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        for i in range(len(self.data_adress)):\n",
        "            if index < self.data_adress[i]:\n",
        "                break\n",
        "          \n",
        "        with open(file = self.files[i-1] , mode='rb') as f:\n",
        "            sample = pickle.load(f)\n",
        "            \n",
        "        sample = self.preprocessing(sample)\n",
        "        sample = self.split_data(sample)\n",
        "          \n",
        "        return sample[index - self.data_adress[i-1],:]\n",
        "          \n",
        "    def __len__(self):\n",
        "        return self.data_adress[-1]"
      ],
      "metadata": {
        "id": "27kv5Q5LTLgr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class concat_dataset():\n",
        "    def __init__(self, data_dic,seq_len):\n",
        "        self.data_dic = data_dic #data_dic : {'dataset1_name : [dataset1_adress],,,datasetN_name : [datasetN_adress]}\n",
        "        self.seq_len = seq_len\n",
        "    \n",
        "    def tr_val_te_split(self,data_list):\n",
        "        train, test = train_test_split(data_list, test_size=0.2) #, shuffle=True, random_state=34), #stratify=target\n",
        "        train, val = train_test_split(train, test_size=0.25) #,shuffle=True, random_state=34)\n",
        "        del data_list\n",
        "        print('split done')\n",
        "        return train, val, test    \n",
        "    \n",
        "    def call(self):\n",
        "        # train_dataset = [] # extend로 빈 어레의 받으면 메모리가 터지는 문제 발생했음.\n",
        "        # val_dataset = []\n",
        "        # test_dataset = []\n",
        "    \n",
        "        for name, data_list in self.data_dic.items():\n",
        "            print(name)\n",
        "            tr, val, te = self.tr_val_te_split(data_list)\n",
        "            \n",
        "            if name =='Sleep_edf':\n",
        "                sleepedf_train_data = Sleepedf_dataset(tr,self.seq_len)\n",
        "                print('sleep train done')\n",
        "                sleepedf_val_data = Sleepedf_dataset(val,self.seq_len)\n",
        "                print('sleep val done')\n",
        "                sleepedf_test_data = Sleepedf_dataset(te,self.seq_len)\n",
        "                print('sleep test done')\n",
        "            \n",
        "            elif name == 'MASS':\n",
        "                MASS_train_data = Sleepedf_dataset(tr,self.seq_len)\n",
        "                print('MASS train done')\n",
        "                MASS_val_data = Sleepedf_dataset(val,self.seq_len)\n",
        "                print('MASS val done')\n",
        "                MASS_test_data = Sleepedf_dataset(te,self.seq_len)\n",
        "                print('MASS test done')\n",
        "        \n",
        "            # train_dataset.extend(train_data)\n",
        "            # val_dataset.extend(val_data)\n",
        "            # test_dataset.extend(test_data)\n",
        "            # print(train_data)\n",
        "\n",
        "        # del train_data,val_data, test_data\n",
        "    \n",
        "        train_dataset = torch.utils.data.ConcatDataset([sleepedf_train_data,MASS_train_data])\n",
        "        val_dataset = torch.utils.data.ConcatDataset([sleepedf_test_data,MASS_val_data])\n",
        "        test_dataset = torch.utils.data.ConcatDataset([sleepedf_val_data,MASS_test_data])\n",
        "    \n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "        "
      ],
      "metadata": {
        "id": "QfZRchAbTLjC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob \n",
        "sleepedf_list = glob.glob('/content/drive/MyDrive/sleep_edfx/sleep_edfx_CT+SC/**')\n",
        "print(len(sleepedf_list))\n",
        "\n",
        "MASS_list = glob.glob('/content/drive/MyDrive/EEG_data/MASS/**')\n",
        "print(len(MASS_list))\n",
        "\n",
        "data_dic = {'MASS' : MASS_list, 'Sleep_edf': sleepedf_list}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxZ4al1kTmqJ",
        "outputId": "78c09921-c28e-4952-e340-0adb78c28bda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MASS"
      ],
      "metadata": {
        "id": "FXxmjfbhRsBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data = MASS_dataset(MASS_list,3000)"
      ],
      "metadata": {
        "id": "blLN0GtLRiUF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sleep-edf"
      ],
      "metadata": {
        "id": "OIX4wIcNj9x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "trainLoader = DataLoader(data, batch_size = 1, shuffle=True)"
      ],
      "metadata": {
        "id": "wPRqPlirRo5o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전에는 sleep edf 10개의 sample만 올려도 메모리 터졌는데, 153개 올려도 RAM 적게 차지."
      ],
      "metadata": {
        "id": "06U2y4QukFw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ConcatDataset을 이용하여 두 dataset 모으는 법"
      ],
      "metadata": {
        "id": "sCi3lalPkbA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = Sleepedf_dataset(sleepedf_list[:20],3000)"
      ],
      "metadata": {
        "id": "R5a_QB9ROTzi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.ConcatDataset([data,data2])"
      ],
      "metadata": {
        "id": "dE0a4J9uPZVt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "trainLoader = DataLoader(train_dataset, batch_size = 2)"
      ],
      "metadata": {
        "id": "E7J4hJbKQcBp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_T = time.time()\n",
        "for batch_idx, batch in enumerate(trainLoader):\n",
        "  print('batch_idx:',batch_idx,' ',batch.shape)\n",
        "  end_T = time.time()\n",
        "  print('time:', end_T - start_T)\n",
        "  if batch_idx >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "ZLwoBLlAQlk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에서 정의한 class 이용해서 dataset concatenate"
      ],
      "metadata": {
        "id": "tzxXsXmsko-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sleepedf_list = glob.glob('/content/drive/MyDrive/sleep_edfx/sleep_edfx_CT+SC/**')\n",
        "print(len(sleepedf_list))\n",
        "\n",
        "MASS_list = glob.glob('/content/drive/MyDrive/EEG_data/MASS/**')\n",
        "print(len(MASS_list))\n",
        "\n",
        "data_dic = {'MASS' : MASS_list, 'Sleep_edf': sleepedf_list}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JpO-UslXe9k",
        "outputId": "d0c27cfd-989c-4f62-f902-9fe57b0dde86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = concat_dataset(data_dic, seq_len = 3000).call()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3FNbW37T-G6",
        "outputId": "6df8d569-4d82-4067-8c2c-58234f523d0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MASS\n",
            "split done\n",
            "MASS train done\n",
            "MASS val done\n",
            "MASS test done\n",
            "Sleep_edf\n",
            "split done\n",
            "sleep train done\n",
            "sleep val done\n",
            "sleep test done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "trainLoader = DataLoader(train_dataset, batch_size = 2, shuffle=True)"
      ],
      "metadata": {
        "id": "HyHl1g4dXYdc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_T = time.time()\n",
        "for batch_idx, batch in enumerate(trainLoader):\n",
        "  print('batch_idx:',batch_idx,' ',batch.shape)\n",
        "  end_T = time.time()\n",
        "  print('time:', end_T - start_T)\n",
        "  if batch_idx >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "Av7TiMQ2XdUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "122de08a-08c4-4d49-fbf8-a57094fe303e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_idx: 0   torch.Size([2, 3000])\n",
            "time: 1021.0426411628723\n",
            "batch_idx: 1   torch.Size([2, 3000])\n",
            "time: 1573.0411252975464\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f5bcb4003d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_idx:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mend_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7a95dc0c2382>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7a95dc0c2382>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# upsampling to 200Hz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-236>\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, up, down, npad, axis, window, n_jobs, pad, verbose)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/filter.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m             y[xi] = _fft_resample(x_, new_len, npads, to_removes,\n\u001b[0;32m-> 1519\u001b[0;31m                                   cuda_dict, pad)\n\u001b[0m\u001b[1;32m   1520\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m         y = parallel(p_fun(x_, new_len, npads, to_removes, cuda_dict, pad)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/mne/cuda.py\u001b[0m in \u001b[0;36m_fft_resample\u001b[0;34m(x, new_len, npads, to_removes, cuda_dict, pad)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mshorter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muse_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshorter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mx_fft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rfft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_len\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mnyq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/fft/_backend.py\u001b[0m in \u001b[0;36m__ua_function__\u001b[0;34m(method, args, kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/fft/_pocketfft/basic.py\u001b[0m in \u001b[0;36mr2c\u001b[0;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Note: overwrite_x is not utilised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAM문제 없이 데이터가 잘 로드 되지만, 데이터 길이가 길어서 preprocessing 하는데 시간이 너무 오래 걸림. --> 데이터 찾을 때마다 preprocessing하는 꼴\n",
        "\n",
        "해결책, data를 짧게 짜르기(ex 30s) or 전처리한 파일로 저장 ( 적어도 sampling rate 통일 시켜서, resampling 하는데 시간이 오래 걸림)\n"
      ],
      "metadata": {
        "id": "erljlyovjQPO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKN1NgXXjOdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}