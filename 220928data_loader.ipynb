{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79d649f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "MASS_dir = sorted(glob.glob('/Users/josangmin/EEG/MASS_SS1/**'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5938df7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MASS_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f15a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pickle\n",
    "import mne\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class EEGLoader(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "        self.sequence_length = 200\n",
    "        \n",
    "        # sample을 split해줬을 때 몇개로 split되는지 누적해서 저장, i번째 data를 찾을 때 data_adress 각 값이 기준이 됨\n",
    "        data_adress = [0]\n",
    "        ad = 0\n",
    "        for i in range(len(self.files)):\n",
    "          with open(file = files[i] , mode='rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "          c,t = sample.shape\n",
    "          t = int(c*t*2/self.sequence_length)\n",
    "          ad += t\n",
    "          \n",
    "          data_adress.append(ad)\n",
    "        print(data_adress[:100])\n",
    "        self.data_adress = data_adress\n",
    "\n",
    "\n",
    "    def preprocessing_sleepedf(self,data):\n",
    "        data = mne.filter.resample(data, up = 2.0) # upsampling to 200Hz  \n",
    "        return data\n",
    "\n",
    "    def preprocessing_MASS(self,data):\n",
    "        data = mne.filter.resample(data, down = 1.28) # downsampling to 200Hz  \n",
    "        return data\n",
    "\n",
    "    def split_data(self, data):\n",
    "        L = self.sequence_length\n",
    "        channels, length = data.shape\n",
    "        a = L*int(length/L)\n",
    "        \n",
    "        if length == a:\n",
    "          data = np.reshape(data,(int(length/L*channels),1,L))\n",
    "        \n",
    "        else:\n",
    "          data = data[:,:a]\n",
    "          data = np.reshape(data,(int(a/L*channels),1,L))\n",
    "        data = np.squeeze(data,1)\n",
    "        return data\n",
    "    \n",
    "    # def get_index(self):  \n",
    "    #     data_adress = []\n",
    "    #     for i in range(len(self.files)):\n",
    "    #       sample = pickle.load(open(self.files[i]))\n",
    "    #       _,t = sample.shape\n",
    "    #       ad = int(t*2/self.sequence_length)*self.sequence_length\n",
    "    #       data_adress.append(ad)\n",
    "    #     return data_adress\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         start_T = time.time()\n",
    "          for i in range(len(self.data_adress)):\n",
    "            if index < self.data_adress[i]:\n",
    "              break\n",
    "          \n",
    "          with open(file = self.files[i-1] , mode='rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "          \n",
    "          \n",
    "          sample1 = self.preprocessing_sleepedf(sample)\n",
    "          split_sample = self.split_data(sample1)\n",
    "          \n",
    "#         end_T = time.time()\n",
    "#         print('time:', end_T - start_T)\n",
    "          return split_sample[index - self.data_adress[i-1],:]\n",
    "          \n",
    "    def __len__(self):\n",
    "        return self.data_adress[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83583684",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASS_dir = MASS_dir[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "123e0108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1526353, 3106804, 4530952, 6099330, 7626236]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainLoader = DataLoader(EEGLoader(MASS_dir), batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90c60422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0   torch.Size([1, 200])\n",
      "time: 140.19377517700195\n",
      "batch_idx: 1   torch.Size([1, 200])\n",
      "time: 244.10474014282227\n",
      "batch_idx: 2   torch.Size([1, 200])\n",
      "time: 362.50570607185364\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_T = time.time()\n",
    "for batch_idx, batch in enumerate(trainLoader):\n",
    "  print('batch_idx:',batch_idx,' ',batch.shape)\n",
    "  end_T = time.time()\n",
    "  print('time:', end_T - start_T)\n",
    "  if batch_idx >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a065d",
   "metadata": {},
   "source": [
    "Sleep data의 경우는 한 smaple당 데이터가 너무 커서 load,split, preprocessing할 때 너무 오래 걸림.\n",
    "그래도 원하는 사이즈의 인풋으로 받을 수 있음.\n",
    "Sleep data는 30초씩 잘라서 저장할 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11812ce",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62fb550",
   "metadata": {},
   "source": [
    "sleep edf 30초씩 잘라서 저장한 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f305c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "sleepedf_list = glob.glob('/Users/josangmin/EEG/Sleep_edfx_30s/**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0e9140cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416272\n"
     ]
    }
   ],
   "source": [
    "print(len(sleepedf_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7fd7e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 60, 120, 180, 240, 300, 360, 420, 480, 540, 600, 660, 720, 780, 840, 900, 960, 1020, 1080, 1140, 1200, 1260, 1320, 1380, 1440, 1500, 1560, 1620, 1680, 1740, 1800, 1860, 1920, 1980, 2040, 2100, 2160, 2220, 2280, 2340, 2400, 2460, 2520, 2580, 2640, 2700, 2760, 2820, 2880, 2940, 3000, 3060, 3120, 3180, 3240, 3300, 3360, 3420, 3480, 3540, 3600, 3660, 3720, 3780, 3840, 3900, 3960, 4020, 4080, 4140, 4200, 4260, 4320, 4380, 4440, 4500, 4560, 4620, 4680, 4740, 4800, 4860, 4920, 4980, 5040, 5100, 5160, 5220, 5280, 5340, 5400, 5460, 5520, 5580, 5640, 5700, 5760, 5820, 5880, 5940]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainLoader = DataLoader(EEGLoader(sleepedf_list), batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "baf8d3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0   torch.Size([256, 200])\n",
      "time: 8.319709300994873\n",
      "batch_idx: 1   torch.Size([256, 200])\n",
      "time: 12.694368124008179\n",
      "batch_idx: 2   torch.Size([256, 200])\n",
      "time: 17.293900966644287\n",
      "batch_idx: 3   torch.Size([256, 200])\n",
      "time: 22.151576042175293\n",
      "batch_idx: 4   torch.Size([256, 200])\n",
      "time: 26.6968891620636\n",
      "batch_idx: 5   torch.Size([256, 200])\n",
      "time: 31.25409507751465\n",
      "batch_idx: 6   torch.Size([256, 200])\n",
      "time: 35.35506200790405\n",
      "batch_idx: 7   torch.Size([256, 200])\n",
      "time: 39.457611083984375\n",
      "batch_idx: 8   torch.Size([256, 200])\n",
      "time: 43.430830001831055\n",
      "batch_idx: 9   torch.Size([256, 200])\n",
      "time: 47.56143832206726\n",
      "batch_idx: 10   torch.Size([256, 200])\n",
      "time: 51.451027154922485\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_T = time.time()\n",
    "for batch_idx, batch in enumerate(trainLoader):\n",
    "  print('batch_idx:',batch_idx,' ',batch.shape)\n",
    "  end_T = time.time()\n",
    "  print('time:', end_T - start_T)\n",
    "  if batch_idx >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf628342",
   "metadata": {},
   "source": [
    "sleep edf 6개 sample 올려도 메모리 터지는 문제,\n",
    "416272개가 153개 sample을 30초 씩 잘라서 저장한 건데, 그것의 절반인 20만개 올려도 메모리 터지지 않음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
