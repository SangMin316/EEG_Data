{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDWt74XAgpLK",
        "outputId": "41a2ae92-3aa7-47ce-862c-0db2edc78163"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l4YUYhC7E4Ck"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pickle\n",
        "import mne\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Sleepedf_dataset(Dataset):\n",
        "    def __init__(self, files):\n",
        "        self.files = files\n",
        "        max_value = 0.\n",
        "        min_value = 0.\n",
        "\n",
        "        for i in range(len(self.files)):\n",
        "            sample = np.load(files[i])['x']\n",
        "            temp_max = sample.max()\n",
        "            temp_min = sample.min()\n",
        "            max_value = np.max([max_value, temp_max])\n",
        "            min_value = np.min([min_value, temp_min])\n",
        "        \n",
        "\n",
        "        self.max_value = max_value\n",
        "        self.min_value = min_value\n",
        "\n",
        "    def preprocessing(self, data):\n",
        "        data_max = np.max(data,axis = 1, keepdims=True) # max value of each channels\n",
        "        data_min = np.min(data,axis = 1, keepdims=True) # shape = c,t\n",
        "        c,t = data.shape\n",
        "        \n",
        "        return data/data_max*np.ones((c,t)) - (data_max - data_min)*np.ones((c,t))/(self.max_value - self.min_value)\n",
        "\n",
        "    def one_hot_encoding(self,y):\n",
        "        if y == '1':\n",
        "          y = np.array([0,1,0,0,0,0])\n",
        "        elif y == '2':\n",
        "          y = np.array([0,0,1,0,0,0])   \n",
        "        elif y == '3':\n",
        "          y = np.array([0,0,0,1,0,0])\n",
        "        elif y == '4':\n",
        "          y = np.array([0,0,0,0,1,0])\n",
        "        elif y == 'R':\n",
        "          y = np.array([0,0,0,0,0,1])\n",
        "        else: # W\n",
        "          y = np.array([1,0,0,0,0,0])      \n",
        "        return y  \n",
        "\n",
        "\n",
        "    def __getitem__(self, index):          \n",
        "        sample = np.load(self.files[index])  \n",
        "        y = self.one_hot_encoding(sample['y'])\n",
        "        sample = self.preprocessing(sample['x'])\n",
        "        return { 'x' : torch.tensor(sample), \n",
        "                 'y' : torch.tensor(y)\n",
        "                   }\n",
        "          \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIOrsdt3_1Yd",
        "outputId": "6f27891b-eb0f-4a84-bbbc-57c30efc8d22"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk5UWrymWlfa",
        "outputId": "aac190c0-1aa5-4f70-9a7e-7204c8a54059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "5283\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "SleepEDF_file_list = glob.glob('/content/drive/MyDrive/sleep_edfx/Cutted_EEG/**')\n",
        "print(len(SleepEDF_file_list))\n",
        "SleepEDF_list = []\n",
        "for i in range(len(SleepEDF_file_list)):\n",
        "    SleepEDF_list.extend(glob.glob(SleepEDF_file_list[i]+'/**'))\n",
        "print(len(SleepEDF_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(SleepEDF_list, test_size=0.2)#, shuffle=True, random_state=34), #stratify=target\n",
        "train, val = train_test_split(train, test_size= 0.25)#, shu"
      ],
      "metadata": {
        "id": "8hZHBofhD_ti"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Sleepedf_dataset(train)\n",
        "val_dataset = Sleepedf_dataset(val)\n",
        "test_dataset = Sleepedf_dataset(test)"
      ],
      "metadata": {
        "id": "eY3pJkIbWoFC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install separableconv-torch"
      ],
      "metadata": {
        "id": "834tX7V_uCyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1463ba0-c3e3-4484-c35f-f4b26ff46c8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: separableconv-torch in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from separableconv-torch) (1.21.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from separableconv-torch) (0.13.1+cu113)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from separableconv-torch) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->separableconv-torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->separableconv-torch) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->separableconv-torch) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from mne.filter import filter_data, notch_filter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, fs, encode_info):\n",
        "        super(Encoder, self).__init__()\n",
        "        #spectral layer means spectral convolution\n",
        "        #self.bac_layer is consist of several SeparableConv2d, which plays the role of temporal separable convolution\n",
        "        #convolution layer are initiated by xavier_uniform initization\n",
        "        #Input are Normalized by self.bn(=torch.nn.BatchNorm2d)\n",
        "        #[batch, electrode, length] -> [batch, electrode, Feature]\n",
        "        self.fs = fs\n",
        "        self.elu = nn.ELU()\n",
        "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.bn = nn.BatchNorm1d(1)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "        self.spectral_layer = nn.Conv1d(1, 10, int(self.fs/2), padding=\"same\")\n",
        "\n",
        "        # self.bac_layer = nn.Sequential()\n",
        "        # for i, arg in enumerate(encode_info):\n",
        "        #     input_dim, output_dim, kernel_size = arg\n",
        "        #     self.bac_layer.add_module(\"temporal_conv_\"+str(i),\n",
        "        #                           nn.Conv1d(input_dim, output_dim, kernel_size, padding = 'same'))\n",
        "        #     self.bac_layer.add_module(\"ELU\",nn.ELU()) \n",
        "        \n",
        "        self.conv1t = nn.Conv1d(10,16, 30, padding ='same')\n",
        "        self.conv2t = nn.Conv1d(16,32, 15, padding ='same')\n",
        "        self.conv3t = nn.Conv1d(32,64, 5, padding ='same')\n",
        "        \n",
        "        torch.nn.init.xavier_uniform_(self.spectral_layer.weight)\n",
        "        #self.bac_layer.apply(weight_init_xavier_uniform)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.spectral_layer(x))\n",
        "        x = self.activation(self.conv1t(x))\n",
        "        x = self.activation(self.conv2t(x))\n",
        "        x = self.activation(self.conv3t(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "#Linear layer for SSL classification\n",
        "class Head_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Head_NN, self).__init__()        \n",
        "        # self.layer = nn.Sequential(\n",
        "        #     nn.Dropout(0.5),\n",
        "        #     nn.Linear(64*length, 5)\n",
        "        # )\n",
        "        \n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 5)\n",
        "        )\n",
        "        \n",
        "        self.softmax = torch.nn.Softmax()\n",
        "        # self.layer.apply(weight_init_xavier_uniform)\n",
        "        self.bn = nn.BatchNorm1d(64)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.mean(x, axis = 2) # Global average pooling into temporal dimension\n",
        "        # x = self.flatten(x)\n",
        "        x = self.layer(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "class StoppedBandPathway(nn.Module):\n",
        "    def __init__(self, fs, Unsupervise, encode_info, bands):\n",
        "        super(StoppedBandPathway, self).__init__()\n",
        "        self.encoder = Encoder(fs, encode_info)\n",
        "        self.pretrain = Head_NN()\n",
        "        self.Unsupervise = Unsupervise\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.pretrain(x)\n",
        "        return x\n",
        "\n",
        "    def getRep(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "glstHzJT3A0Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class feature_extractor3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(feature_extractor3,self).__init__()\n",
        "        \n",
        "        self.channels = 1 # we use only single channel \n",
        "        \n",
        "        # Activation functions\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        # self.bn = nn.BatchNorm1d(1)\n",
        "\n",
        "        # self.conv2t = nns.SeparableConv1d(16,32,10,padding ='same') (in_channels, out_channels, kernel size,,,) \n",
        "\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.conv1t = nn.Conv1d(1,10, 30, padding ='same') #in_channels, out_channels, kernel_size, \n",
        "        self.conv1s = nn.Conv1d(10,10,self.channels)\n",
        "        self.conv2t = nn.Conv1d(10,20,15,padding ='same') \n",
        "        self.conv2s = nn.Conv1d(20,20,self.channels)\n",
        "        self.conv3t = nn.Conv1d(20,34,5,padding ='same')\n",
        "        self.conv3s = nn.Conv1d(34,34,self.channels)\n",
        "        \n",
        "                  \n",
        "        # Flatteninig\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Decision making\n",
        "        self.Linear = nn.Linear(256000 ,4) # \n",
        "\n",
        "      \n",
        "    def embedding(self, x):\n",
        "        x = self.activation(self.conv1t(x))\n",
        "        f1 = self.activation(self.conv1s(x))\n",
        "\n",
        "        x = self.activation(self.conv2t(x))\n",
        "        f2 = self.activation(self.conv2s(x))\n",
        "\n",
        "        x = self.activation(self.conv3t(x))\n",
        "        f3 = self.activation(self.conv3s(x))\n",
        "        \n",
        "        # multi-scale feature representation by exploiting intermediate features\n",
        "        feature = torch.cat([f1, f2, f3],dim = 1 )\n",
        "        \n",
        "        return feature\n",
        "\n",
        "    def classifier(self, feature):\n",
        "        # Flattening, dropout, mapping into the decision nodes\n",
        "        feature = self.flatten(feature)\n",
        "        feature = self.dropout(feature)\n",
        "        y_hat = self.softmax(self.Linear(feature))\n",
        "        return y_hat    \n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.embedding(x)\n",
        "        y_hat = self.classifier(feature)\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "17Wx4RicDlgZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# # import separableconv.nn as snn\n",
        "\n",
        "# class FeatureEncoder(nn.Module):\n",
        "#     def __init__(self,x1,x2):\n",
        "#         super(FeatureEncoder, self).__init__()\n",
        "#         self.sepctral_rept = x1\n",
        "#         self.temporal_rept = x2\n",
        "#         self.conv1 = nn.Conv2d(64,64, (2,1))\n",
        "#         self.conv2 = nn.Conv2d(64,64, (2,1))\n",
        "\n",
        "\n",
        "#     def fGAP(self, x):\n",
        "#         x = torch.tensor(x)\n",
        "#         x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "#         return x\n",
        "\n",
        "#     def fGVP(self, x):\n",
        "#          #  x.shape = batch,  channel, feature, time -->\n",
        "#         x = torch.tensor(x)\n",
        "#         torch.var(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "#         return x    \n",
        "    \n",
        "\n",
        "#     def forward(self,b,c,f,t):\n",
        "#         f_1 = self.sepctral_rept\n",
        "#         f_2 = self.temporal_rept\n",
        "#         f_1 = torch.reshape(f_1,(b,f,c,t)) \n",
        "#         f_2 = torch.reshape(f_2,(b,f,c,t)) \n",
        "#         f_1 = self.conv1(f_1)\n",
        "#         f_2 = self.conv2(f_2) # b,f,1, t\n",
        "        \n",
        "#         # print(f_1.shape,f_2.shape)\n",
        "#         f_GAP = torch.cat(( self.fGAP(f_1).squeeze(),self.fGAP(f_2).squeeze() ), axis=1)\n",
        "#         f_GVP = torch.cat((self.fGVP(f_1).squeeze(),self.fGVP(f_2).squeeze()), axis=1)\n",
        "#         # print(f_GAP.shape,f_GVP.shape)\n",
        "\n",
        "\n",
        "#         return  f_GAP, f_GVP\n",
        "\n",
        "\n",
        "# class StatisticianModule(nn.Module):\n",
        "#     def __init__(self, dense, classes):\n",
        "#         super(StatisticianModule, self).__init__()\n",
        "#         self.classes = classes\n",
        "\n",
        "#         self.softmax = torch.nn.Softmax(dim=1)\n",
        "#         self.c_dense = nn.Linear(dense, 64*2*2) # 64*2*2\n",
        "\n",
        "#         self.gap_pwconv = nn.Conv1d(128, dense, 1) \n",
        "#         self.gvp_pwconv = nn.Conv1d(128, dense, 1)\n",
        "\n",
        "#         self.fullconnect = nn.Linear(dense, self.classes)\n",
        "\n",
        "\n",
        "#     def forward(self, f_GAP, f_GVP):\n",
        "#         #[batch, gap+gvp] -> [batch,dense]\n",
        "      \n",
        "#         c = self.softmax(self.c_dense(torch.cat((f_GAP, f_GVP),axis=1)))\n",
        "#         # print('c:',c.shape)\n",
        "#         #[batch, gap, 1] -> [batch, 1, dense] -> [batch, dense]\n",
        "#         f_GAP_d = self.gap_pwconv(f_GAP.unsqueeze(dim=-1)).squeeze()\n",
        "#         f_GVP_d = self.gvp_pwconv(f_GVP.unsqueeze(dim=-1)).squeeze()\n",
        "        \n",
        "#         f_GAP_dd = torch.sum(c*f_GAP_d,dim=1)\n",
        "#         f_GVP_dd = torch.sum(c*f_GVP_d,dim=1)\n",
        "#         # print('f_GAP_dd: ',f_GAP_dd.shape)\n",
        "#         ALN = torch.div(torch.sub(f_GAP_d.T,f_GAP_dd),f_GAP_dd).T\n",
        "#         # print('ALN', ALN.shape)\n",
        "\n",
        "#         y_hat = self.fullconnect(ALN)\n",
        "#         # print('y_hat: ',y_hat.shape)\n",
        "#         return y_hat"
      ],
      "metadata": {
        "id": "h3bHNu5vCz4v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# import separableconv.nn as snn\n",
        "\n",
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self,x1,x2):\n",
        "        super(FeatureEncoder, self).__init__()\n",
        "        self.sepctral_rept = x1\n",
        "        self.temporal_rept = x2\n",
        "        self.conv1 = nn.Conv2d(64,64, (2,1))\n",
        "        self.conv2 = nn.Conv2d(64,64, (2,1))\n",
        "\n",
        "\n",
        "    def fGAP(self, x):\n",
        "        x = torch.tensor(x)\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        return x\n",
        "\n",
        "    def fGVP(self, x):\n",
        "        x = torch.tensor(x)\n",
        "        torch.var(x.view(x.size(0), x.size(1), -1), dim=2)\n",
        "        return x    \n",
        "    \n",
        "\n",
        "    def forward(self,b,c,f,t):\n",
        "        f_1 = self.sepctral_rept\n",
        "        f_2 = self.temporal_rept\n",
        "        f_1 = torch.reshape(f_1,(b,f,c,t)) \n",
        "        f_2 = torch.reshape(f_2,(b,f,c,t)) \n",
        "        \n",
        "        # print(f_1.shape,f_2.shape)\n",
        "        f_GAP = torch.cat(( self.fGAP(f_1).squeeze(),self.fGAP(f_2).squeeze() ), axis=1)\n",
        "        f_GVP = torch.cat((self.fGVP(f_1).squeeze(),self.fGVP(f_2).squeeze()), axis=1)\n",
        "        print(f_GAP.shape,f_GVP.shape)\n",
        "\n",
        "\n",
        "        return  f_GAP, f_GVP\n",
        "\n",
        "\n",
        "class StatisticianModule(nn.Module):\n",
        "    def __init__(self, dense, classes):\n",
        "        super(StatisticianModule, self).__init__()\n",
        "        self.classes = classes\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(64,64, (2,1))\n",
        "        self.conv2 = nn.Conv2d(64,64, (2,1))\n",
        "\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.c_dense = nn.Linear(dense, 64*2*2) # 64*2*2\n",
        "\n",
        "        self.gap_pwconv = nn.Conv1d(128, dense, 1) \n",
        "        self.gvp_pwconv = nn.Conv1d(128, dense, 1)\n",
        "\n",
        "        self.fullconnect = nn.Linear(dense, self.classes)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.c_dense.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.gap_pwconv.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.gvp_pwconv.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fullconnect.weight)\n",
        "        \n",
        "\n",
        "    def GAPGVP(self, x1, x2, b,c,f,t ):\n",
        "        x1 = torch.reshape(x1,(b,f,c,t)) \n",
        "        x2 = torch.reshape(x2,(b,f,c,t)) \n",
        "        x1 = self.conv1(x1)\n",
        "        x2 = self.conv2(x2)\n",
        "\n",
        "        f_GAP = torch.cat((F.adaptive_avg_pool2d(x1, (1, 1)).squeeze(), F.adaptive_avg_pool2d(x2, (1, 1)).squeeze()), axis=1)\n",
        "        f_GVP = torch.cat((torch.var(x1.view(x1.size(0), x1.size(1), -1), dim=2),torch.var(x1.view(x1.size(0), x1.size(1), -1), dim=2)), axis=1)\n",
        "        del x1\n",
        "        del x2\n",
        "        return f_GAP, f_GVP\n",
        "\n",
        "    def forward(self,x1, x2, b,c,f,t):\n",
        "        f_GAP, f_GVP = self.GAPGVP(x1, x2, b,c,f,t )\n",
        "        # print(f_GAP.shape, f_GVP.shape)\n",
        "        c = self.softmax(self.c_dense(torch.cat((f_GAP, f_GVP),axis=1)))\n",
        "        # print('c:',c.shape)\n",
        "        #[batch, gap, 1] -> [batch, 1, dense] -> [batch, dense]\n",
        "        f_GAP_d = self.gap_pwconv(f_GAP.unsqueeze(dim=-1)).squeeze()\n",
        "        f_GVP_d = self.gvp_pwconv(f_GVP.unsqueeze(dim=-1)).squeeze()\n",
        "        \n",
        "        f_GAP_dd = torch.sum(c*f_GAP_d,dim=1)\n",
        "        f_GVP_dd = torch.sum(c*f_GVP_d,dim=1)\n",
        "        # print('f_GAP_dd: ',f_GAP_dd.shape)\n",
        "        ALN = torch.div(torch.sub(f_GAP_d.T,f_GAP_dd),f_GAP_dd).T\n",
        "        # print('ALN', ALN.shape)\n",
        "\n",
        "        y_hat = self.fullconnect(ALN)\n",
        "        # print('y_hat: ',y_hat.shape)\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "t8IC9wl5vk_V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import separableconv.nn as nn\n",
        "\n",
        "# class FeatureEncoder(nn.Module):\n",
        "#     def __init__(self,x1,x2):\n",
        "#         super(FeatureEncoder, self).__init__()\n",
        "#         self.sepctral_rept = x1\n",
        "#         self.temporal_rept = x2\n",
        "#         self.conv = nn.Conv2d(1,16, (1,2), padding ='same')\n",
        "#         #self.GVP = torch.var(dim=-1)\n",
        "\n",
        "#     def Temporal_GAP(self, x):\n",
        "#          #  x.shape = batch*channel, feature, time -->\n",
        "#         x = torch.tensor(x)\n",
        "#         x = torch.mean(x, 1) # channel\n",
        "#         x = torch.mean(x, 2) # time\n",
        "#         return x\n",
        "\n",
        "#     def Temporal_GVP(self, x):\n",
        "#          #  x.shape = batch,  channel, feature, time -->\n",
        "#         x = torch.tensor(x)\n",
        "#         x = torch.var(x, 1) # channel\n",
        "#         x = torch.var(x, 2) # time\n",
        "#         return x    \n",
        "\n",
        "#     def forward(self,b,c,f,t):\n",
        "#         f_1 = self.sepctral_rept\n",
        "#         f_2 = self.temporal_rept\n",
        "#         f_1 = torch.reshape(f_1,(b,c,f,t))\n",
        "#         f_2 = torch.reshape(f_2,(b,c,f,t))\n",
        "#         f_1 = self.conv(f_1)\n",
        "#         f_2 = self.conv(f_2)\n",
        "\n",
        "        \n",
        "#         f_GAP = torch.cat(( self.Temporal_GAP(f_1), self.Temporal_GAP(f_2)), axis=1)\n",
        "#         f_GVP = torch.cat((self.Temporal_GVP(f_1), self.Temporal_GVP(f_2)), axis=1)\n",
        "      \n",
        "#         # f_GAP = torch.cat((self.GAP(f_1).squeeze(), self.GAP(f_2).squeeze(), self.Temporal_GAP(f_3).squeeze()), axis=1)\n",
        "#         # f_GVP = torch.cat((torch.var(f_1, dim=-1), torch.var(f_2, dim=-1), torch.var(f_3, dim=-1)), axis=1)\n",
        "\n",
        "#         return  f_GAP, f_GVP\n",
        "\n",
        "\n",
        "# class StatisticianModule(nn.Module):\n",
        "#     def __init__(self, dense, classes):\n",
        "#         super(StatisticianModule, self).__init__()\n",
        "#         self.classes = classes\n",
        "\n",
        "#         self.softmax = torch.nn.Softmax(dim=1)\n",
        "#         self.c_dense = nn.Linear(dense, 64*2*2)\n",
        "\n",
        "#         self.gap_pwconv = nn.Conv1d(64*2, dense, 1)\n",
        "#         self.gvp_pwconv = nn.Conv1d(64*2, dense, 1)\n",
        "\n",
        "#         self.fullconnect = nn.Linear(dense, self.classes)\n",
        "\n",
        "\n",
        "#     def forward(self, f_GAP, f_GVP):\n",
        "#         #[batch, gap+gvp] -> [batch,dense]\n",
        "      \n",
        "#         c = self.softmax(self.c_dense(torch.cat((f_GAP, f_GVP),axis=1)))\n",
        "#         # print('c:',c.shape)\n",
        "#         #[batch, gap, 1] -> [batch, 1, dense] -> [batch, dense]\n",
        "#         f_GAP_d = self.gap_pwconv(f_GAP.unsqueeze(dim=-1)).squeeze()\n",
        "#         f_GVP_d = self.gvp_pwconv(f_GVP.unsqueeze(dim=-1)).squeeze()\n",
        "        \n",
        "#         f_GAP_dd = torch.sum(c*f_GAP_d,dim=1)\n",
        "#         f_GVP_dd = torch.sum(c*f_GVP_d,dim=1)\n",
        "#         # print('f_GAP_dd: ',f_GAP_dd.shape)\n",
        "#         ALN = torch.div(torch.sub(f_GAP_d.T,f_GAP_dd),f_GAP_dd).T\n",
        "#         # print('ALN', ALN.shape)\n",
        "\n",
        "#         y_hat = self.fullconnect(ALN)\n",
        "#         # print('y_hat: ',y_hat.shape)\n",
        "#         return y_hat"
      ],
      "metadata": {
        "id": "26isRnwBt_1e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load pretrain model"
      ],
      "metadata": {
        "id": "PK5lzwTNxWpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FE1 = torch.load('/content/Spectral__10s_ep6_.pt')\n",
        "FE2 = torch.load('/content/Temporal__10s_ep15_.pt')\n",
        "FE1.eval()\n",
        "FE2.eval()\n"
      ],
      "metadata": {
        "id": "IYeo7tM_Gxsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fffa2bc-ff46-4ba5-fcce-411bf96cbb5d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature_extractor3(\n",
              "  (activation): LeakyReLU(negative_slope=0.01)\n",
              "  (softmax): Softmax(dim=None)\n",
              "  (conv1t): Conv1d(1, 10, kernel_size=(30,), stride=(1,), padding=same)\n",
              "  (conv1s): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
              "  (conv2t): Conv1d(10, 20, kernel_size=(15,), stride=(1,), padding=same)\n",
              "  (conv2s): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
              "  (conv3t): Conv1d(20, 34, kernel_size=(5,), stride=(1,), padding=same)\n",
              "  (conv3s): Conv1d(34, 34, kernel_size=(1,), stride=(1,))\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (Linear): Linear(in_features=128000, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#\"cpu\"\n",
        "\n",
        "print(device)\n"
      ],
      "metadata": {
        "id": "TGeS3Hycs2gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36462de7-ab83-4549-984d-6aeabec358e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "learning_rate = 0.0005\n",
        "batch_size = 128\n",
        "model = StatisticianModule(256,6).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "1Sg0RcOsGxvT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "trainLoader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "valLoader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
        "testLoader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Mg_nmDT_iPbf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for batch_idx, batch in enumerate(SleepEDF_list):\n",
        "#     a = np.load(batch)\n",
        "#     if a['y'] != 'W' and a['y'] !='R' and a['y'] !='1' and a['y'] !='2' and a['y'] !='3' and a['y'] !='4':\n",
        "#       print(a['y'])\n"
      ],
      "metadata": {
        "id": "8pstynYNNnN5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "t3ZnedrL-WDG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_tr = []\n",
        "loss_val = []\n",
        "acc_tr = []\n",
        "acc_val = []\n",
        "for epoch in range(epochs):\n",
        "    loss_ep = 0  # add batch loss in epoch\n",
        "    acc_ep = 0\n",
        "    for batch_idx, batch in enumerate(trainLoader):\n",
        "        optimizer.zero_grad()\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "       \n",
        "        loss = CrossEL(pred, label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step() \n",
        "\n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0] #acc/(batch*channels*4(augmented))\n",
        "        loss_ep += loss.item()\n",
        "        # print('acc:', acc)\n",
        "        acc_ep += acc\n",
        "\n",
        "    loss_tr.append((loss_ep)/len(trainLoader))\n",
        "    acc_tr.append((acc_ep)/len(trainLoader))        \n",
        "\n",
        "    loss_ep_val = 0\n",
        "    acc_ep_val = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(valLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_val += loss.item()\n",
        "        acc_ep_val += acc\n",
        "\n",
        "    loss_val.append((loss_ep_val)/len(valLoader))\n",
        "    acc_val.append((acc_ep_val)/len(valLoader))\n",
        "    print(\"epoch : \", epoch, \"  train loss : \", loss_tr[epoch], 'train acc : ', acc_tr[epoch], \"    val loss : \", loss_val[epoch], 'val acc : ', acc_val[epoch])\n",
        "    torch.save(model,'SleepEDF_noConv_30s_ep' + str(epoch)+'_.pt')\n"
      ],
      "metadata": {
        "id": "eEnhXWVI3so0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffec2dde-6737-4b2b-b07e-a093a7a4c937"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:304: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:882.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  0   train loss :  6.840794084619013 train acc :  0.41885953608247417     val loss :  13.935553957066984 val acc :  0.3383575336700337\n",
            "epoch :  1   train loss :  2.105936612377309 train acc :  0.436694587628866     val loss :  1.6678554033300046 val acc :  0.45817550505050497\n",
            "epoch :  2   train loss :  1.6357516335936015 train acc :  0.4426063144329897     val loss :  1.5575938763595516 val acc :  0.4481797138047138\n",
            "epoch :  3   train loss :  1.50790406716181 train acc :  0.4433440721649485     val loss :  1.448138399355377 val acc :  0.45501893939393934\n",
            "epoch :  4   train loss :  1.4352373966228062 train acc :  0.4566688144329897     val loss :  1.4079170039359492 val acc :  0.4854008838383838\n",
            "epoch :  5   train loss :  1.3983394225899937 train acc :  0.4813563144329897     val loss :  1.3837398913318937 val acc :  0.49429187710437705\n",
            "epoch :  6   train loss :  1.3698884805652898 train acc :  0.49063144329896907     val loss :  1.3390380194636458 val acc :  0.5203598484848485\n",
            "epoch :  7   train loss :  1.3445247072956916 train acc :  0.4987306701030928     val loss :  1.3371182939834165 val acc :  0.5130734427609427\n",
            "epoch :  8   train loss :  1.3231591064772885 train acc :  0.5015818298969072     val loss :  1.3118097819466308 val acc :  0.520675505050505\n",
            "epoch :  9   train loss :  1.3004675567436235 train acc :  0.5129800257731959     val loss :  1.2862712625470578 val acc :  0.5216487794612795\n",
            "epoch :  10   train loss :  1.2823219894655242 train acc :  0.5178930412371134     val loss :  1.2702654947790157 val acc :  0.5303293350168351\n",
            "epoch :  11   train loss :  1.263880808110416 train acc :  0.5245682989690722     val loss :  1.270329267946025 val acc :  0.5289088804713804\n",
            "epoch :  12   train loss :  1.2464746041972306 train acc :  0.5359664948453609     val loss :  1.2355226854031736 val acc :  0.562131734006734\n",
            "epoch :  13   train loss :  1.234324446324086 train acc :  0.5415302835051546     val loss :  1.2196480550337583 val acc :  0.5672348484848485\n",
            "epoch :  14   train loss :  1.2198885303272953 train acc :  0.5545940721649485     val loss :  1.2183529509447817 val acc :  0.5739688552188552\n",
            "epoch :  15   train loss :  1.206325046091254 train acc :  0.5573550257731958     val loss :  1.207991403645541 val acc :  0.567550505050505\n",
            "epoch :  16   train loss :  1.1944991426253404 train acc :  0.5663176546391753     val loss :  1.1983005835679408 val acc :  0.5763362794612795\n",
            "epoch :  17   train loss :  1.1825201931210791 train acc :  0.5670425257731959     val loss :  1.1882308847196432 val acc :  0.5847011784511785\n",
            "epoch :  18   train loss :  1.1737533616714333 train acc :  0.5754059278350515     val loss :  1.1812730816834582 val acc :  0.5754682239057239\n",
            "epoch :  19   train loss :  1.1640018948069148 train acc :  0.5798807989690722     val loss :  1.1798294967785947 val acc :  0.5950126262626263\n",
            "epoch :  20   train loss :  1.1538994095556572 train acc :  0.5856926546391752     val loss :  1.1536793598810349 val acc :  0.6060869107744108\n",
            "epoch :  21   train loss :  1.145409806233005 train acc :  0.5919426546391753     val loss :  1.1538184703471177 val acc :  0.6067445286195287\n",
            "epoch :  22   train loss :  1.137905843073704 train acc :  0.5896681701030928     val loss :  1.1452316848803408 val acc :  0.6046664562289563\n",
            "epoch :  23   train loss :  1.128670604202186 train acc :  0.598791881443299     val loss :  1.1304911795624544 val acc :  0.6106376262626263\n",
            "epoch :  24   train loss :  1.1219424850062092 train acc :  0.5952802835051546     val loss :  1.1422062772528037 val acc :  0.6115056818181819\n",
            "epoch :  25   train loss :  1.1129785692187257 train acc :  0.6025418814432989     val loss :  1.1363785260747759 val acc :  0.6015098905723906\n",
            "epoch :  26   train loss :  1.1073190011238179 train acc :  0.6078930412371134     val loss :  1.1306063633048373 val acc :  0.6117161195286196\n",
            "epoch :  27   train loss :  1.1008564533943013 train acc :  0.6128060567010309     val loss :  1.1163633179069679 val acc :  0.6083491161616161\n",
            "epoch :  28   train loss :  1.0923570633951103 train acc :  0.6111920103092783     val loss :  1.1002659450951682 val acc :  0.6320233585858586\n",
            "epoch :  29   train loss :  1.0867960332310278 train acc :  0.6219168814432989     val loss :  1.1250318600887297 val acc :  0.6127946127946129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "additional learning"
      ],
      "metadata": {
        "id": "_2kCHnP5DCJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30,50):\n",
        "    loss_ep = 0  # add batch loss in epoch\n",
        "    acc_ep = 0\n",
        "    for batch_idx, batch in enumerate(trainLoader):\n",
        "        optimizer.zero_grad()\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "       \n",
        "        loss = CrossEL(pred, label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step() \n",
        "\n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0] #acc/(batch*channels*4(augmented))\n",
        "        loss_ep += loss.item()\n",
        "        # print('acc:', acc)\n",
        "        acc_ep += acc\n",
        "\n",
        "    loss_tr.append((loss_ep)/len(trainLoader))\n",
        "    acc_tr.append((acc_ep)/len(trainLoader))        \n",
        "\n",
        "    loss_ep_val = 0\n",
        "    acc_ep_val = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(valLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_val += loss.item()\n",
        "        acc_ep_val += acc\n",
        "\n",
        "    loss_val.append((loss_ep_val)/len(valLoader))\n",
        "    acc_val.append((acc_ep_val)/len(valLoader))\n",
        "    print(\"epoch : \", epoch, \"  train loss : \", loss_tr[epoch], 'train acc : ', acc_tr[epoch], \"    val loss : \", loss_val[epoch], 'val acc : ', acc_val[epoch])\n",
        "    torch.save(model,'SleepEDF_Conv_30s_ep' + str(epoch)+'_.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L6Ofrl7CtTf",
        "outputId": "2eb1af5f-aa39-4dde-87bf-4122b25c9a4c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  30   train loss :  1.0800243160831886 train acc :  0.6286791237113402     val loss :  1.0968956145538669 val acc :  0.6185553451178452\n",
            "epoch :  31   train loss :  1.0750024316140652 train acc :  0.6221423969072165     val loss :  1.1260892767152437 val acc :  0.6196338383838383\n",
            "epoch :  32   train loss :  1.067705886142794 train acc :  0.6237789948453608     val loss :  1.1168814739524011 val acc :  0.613873106060606\n",
            "epoch :  33   train loss :  1.0632877692432363 train acc :  0.6313917525773196     val loss :  1.0619601843780044 val acc :  0.6500420875420876\n",
            "epoch :  34   train loss :  1.0567146921514283 train acc :  0.6336791237113402     val loss :  1.082949955869303 val acc :  0.6247369528619529\n",
            "epoch :  35   train loss :  1.0504324159016785 train acc :  0.6392912371134021     val loss :  1.0591533008297693 val acc :  0.6281039562289563\n",
            "epoch :  36   train loss :  1.0468415329400114 train acc :  0.6419297680412371     val loss :  1.067629633211885 val acc :  0.6374421296296297\n",
            "epoch :  37   train loss :  1.041819735923014 train acc :  0.6402416237113403     val loss :  1.063767094484347 val acc :  0.654698021885522\n",
            "epoch :  38   train loss :  1.0379933706629234 train acc :  0.6395425257731958     val loss :  1.080717242204291 val acc :  0.6489372895622896\n",
            "epoch :  39   train loss :  1.0303988119249339 train acc :  0.6427545103092784     val loss :  1.0560115209041965 val acc :  0.6530671296296297\n",
            "epoch :  40   train loss :  1.025604319285359 train acc :  0.6504413659793815     val loss :  1.0433530206110029 val acc :  0.650462962962963\n",
            "epoch :  41   train loss :  1.0213475589811822 train acc :  0.6473807989690722     val loss :  1.0499730720191602 val acc :  0.6425452441077442\n",
            "epoch :  42   train loss :  1.016113824057837 train acc :  0.6432796391752578     val loss :  1.0362433053706757 val acc :  0.6601167929292929\n",
            "epoch :  43   train loss :  1.0104960091832602 train acc :  0.656153350515464     val loss :  1.0531845785957106 val acc :  0.6404671717171717\n",
            "epoch :  44   train loss :  1.0057302136003452 train acc :  0.6518911082474227     val loss :  1.0294843266068594 val acc :  0.6657723063973064\n",
            "epoch :  45   train loss :  1.0030603976991657 train acc :  0.6569297680412371     val loss :  1.0349631918454503 val acc :  0.646885521885522\n",
            "epoch :  46   train loss :  0.9980944822580414 train acc :  0.6535921391752578     val loss :  1.0323411041069328 val acc :  0.6576178451178452\n",
            "epoch :  47   train loss :  0.9914919718853126 train acc :  0.6573163659793815     val loss :  1.0127316794179064 val acc :  0.6608796296296297\n",
            "epoch :  48   train loss :  0.9882371163365105 train acc :  0.660528350515464     val loss :  1.0074582371966772 val acc :  0.664246632996633\n",
            "epoch :  49   train loss :  0.983068897111975 train acc :  0.6596907216494846     val loss :  1.0067652651473924 val acc :  0.6710858585858586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50,70):\n",
        "    loss_ep = 0  # add batch loss in epoch\n",
        "    acc_ep = 0\n",
        "    for batch_idx, batch in enumerate(trainLoader):\n",
        "        optimizer.zero_grad()\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "       \n",
        "        loss = CrossEL(pred, label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step() \n",
        "\n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0] #acc/(batch*channels*4(augmented))\n",
        "        loss_ep += loss.item()\n",
        "        # print('acc:', acc)\n",
        "        acc_ep += acc\n",
        "\n",
        "    loss_tr.append((loss_ep)/len(trainLoader))\n",
        "    acc_tr.append((acc_ep)/len(trainLoader))        \n",
        "\n",
        "    loss_ep_val = 0\n",
        "    acc_ep_val = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(valLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_val += loss.item()\n",
        "        acc_ep_val += acc\n",
        "\n",
        "    loss_val.append((loss_ep_val)/len(valLoader))\n",
        "    acc_val.append((acc_ep_val)/len(valLoader))\n",
        "    print(\"epoch : \", epoch, \"  train loss : \", loss_tr[epoch], 'train acc : ', acc_tr[epoch], \"    val loss : \", loss_val[epoch], 'val acc : ', acc_val[epoch])\n",
        "    torch.save(model,'SleepEDF_Conv_30s_ep' + str(epoch)+'_.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvGXt2GZL6WA",
        "outputId": "7d270c77-5e81-4e1d-8fee-919917d129c1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  50   train loss :  0.9795771825689937 train acc :  0.6577545103092785     val loss :  1.013969940758782 val acc :  0.6602220117845118\n",
            "epoch :  51   train loss :  0.9742341388334621 train acc :  0.6701417525773197     val loss :  1.0201740444351501 val acc :  0.6523042929292929\n",
            "epoch :  52   train loss :  0.971978126540084 train acc :  0.6652545103092784     val loss :  1.0465325204576545 val acc :  0.661958122895623\n",
            "epoch :  53   train loss :  0.9666111924847679 train acc :  0.6650289948453608     val loss :  0.9966075917472084 val acc :  0.6609848484848485\n",
            "epoch :  54   train loss :  0.9623616534473313 train acc :  0.6702287371134021     val loss :  1.002456204144532 val acc :  0.6476220538720538\n",
            "epoch :  55   train loss :  0.9606027741801383 train acc :  0.6661920103092784     val loss :  0.9996352540081668 val acc :  0.6644570707070707\n",
            "epoch :  56   train loss :  0.9543278746701508 train acc :  0.6738788659793815     val loss :  0.98593384873648 val acc :  0.6643518518518519\n",
            "epoch :  57   train loss :  0.9524073577334758 train acc :  0.6734922680412372     val loss :  0.9660819360909133 val acc :  0.6828177609427609\n",
            "epoch :  58   train loss :  0.9477118389618093 train acc :  0.6729413659793815     val loss :  0.9735828592303414 val acc :  0.6652199074074074\n",
            "epoch :  59   train loss :  0.942770742632223 train acc :  0.6732667525773196     val loss :  0.9753724487354548 val acc :  0.6746632996632997\n",
            "epoch :  60   train loss :  0.939486590064846 train acc :  0.6780154639175258     val loss :  0.9807000825558289 val acc :  0.6591435185185186\n",
            "epoch :  61   train loss :  0.9376642944537386 train acc :  0.6780798969072166     val loss :  0.9759121391138686 val acc :  0.6560921717171717\n",
            "epoch :  62   train loss :  0.934967032254628 train acc :  0.6759536082474227     val loss :  0.9772808688683146 val acc :  0.6746632996632997\n",
            "epoch :  63   train loss :  0.9294442995915851 train acc :  0.6760663659793815     val loss :  0.9661565431345078 val acc :  0.6661931818181819\n",
            "epoch :  64   train loss :  0.9249440885077174 train acc :  0.6798904639175258     val loss :  0.9583256527875952 val acc :  0.6856323653198654\n",
            "epoch :  65   train loss :  0.9234297600462456 train acc :  0.6836404639175258     val loss :  0.9391264632794621 val acc :  0.6745580808080809\n",
            "epoch :  66   train loss :  0.9180641309264207 train acc :  0.684590850515464     val loss :  0.9668889996192538 val acc :  0.663694234006734\n",
            "epoch :  67   train loss :  0.9156108683242569 train acc :  0.6870779639175258     val loss :  0.9412161680292189 val acc :  0.6806344696969697\n",
            "epoch :  68   train loss :  0.9129849785524685 train acc :  0.6809664948453609     val loss :  0.9481777773245186 val acc :  0.6763994107744108\n",
            "epoch :  69   train loss :  0.9117928056524269 train acc :  0.6889787371134021     val loss :  0.9598229770981378 val acc :  0.6746632996632997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(70,90):\n",
        "    loss_ep = 0  # add batch loss in epoch\n",
        "    acc_ep = 0\n",
        "    for batch_idx, batch in enumerate(trainLoader):\n",
        "        optimizer.zero_grad()\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "       \n",
        "        loss = CrossEL(pred, label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step() \n",
        "\n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0] #acc/(batch*channels*4(augmented))\n",
        "        loss_ep += loss.item()\n",
        "        # print('acc:', acc)\n",
        "        acc_ep += acc\n",
        "\n",
        "    loss_tr.append((loss_ep)/len(trainLoader))\n",
        "    acc_tr.append((acc_ep)/len(trainLoader))        \n",
        "\n",
        "    loss_ep_val = 0\n",
        "    acc_ep_val = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(valLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_val += loss.item()\n",
        "        acc_ep_val += acc\n",
        "\n",
        "    loss_val.append((loss_ep_val)/len(valLoader))\n",
        "    acc_val.append((acc_ep_val)/len(valLoader))\n",
        "    print(\"epoch : \", epoch, \"  train loss : \", loss_tr[epoch], 'train acc : ', acc_tr[epoch], \"    val loss : \", loss_val[epoch], 'val acc : ', acc_val[epoch])\n",
        "    torch.save(model,'SleepEDF_Conv_30s_ep' + str(epoch)+'_.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY8D-qThQDlR",
        "outputId": "5e5a2e01-11a5-4ee8-be19-f513d5b0d6c9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  70   train loss :  0.907995027146619 train acc :  0.6878414948453608     val loss :  0.9342092460901156 val acc :  0.6634837962962963\n",
            "epoch :  71   train loss :  0.9036884698792921 train acc :  0.6848034793814434     val loss :  0.9193046725775379 val acc :  0.6924715909090909\n",
            "epoch :  72   train loss :  0.9036955847234716 train acc :  0.686291881443299     val loss :  0.9291902526177608 val acc :  0.6712962962962963\n",
            "epoch :  73   train loss :  0.8998756029709907 train acc :  0.6910663659793815     val loss :  0.9323506238169674 val acc :  0.6831334175084175\n",
            "epoch :  74   train loss :  0.896046970228427 train acc :  0.6910534793814433     val loss :  0.9172995612016442 val acc :  0.6889993686868687\n",
            "epoch :  75   train loss :  0.8921792911499147 train acc :  0.690715206185567     val loss :  0.9361378723602708 val acc :  0.6732428451178452\n",
            "epoch :  76   train loss :  0.8901244134739037 train acc :  0.6917042525773196     val loss :  0.9211501660256024 val acc :  0.6719539141414141\n",
            "epoch :  77   train loss :  0.8887229180997706 train acc :  0.6922164948453609     val loss :  0.9324737171252373 val acc :  0.6894202441077442\n",
            "epoch :  78   train loss :  0.88606275918018 train acc :  0.6895038659793815     val loss :  0.9293831959198252 val acc :  0.6731376262626263\n",
            "epoch :  79   train loss :  0.8835377225288629 train acc :  0.6932538659793814     val loss :  0.9254659148451899 val acc :  0.6791087962962963\n",
            "epoch :  80   train loss :  0.8779916785411553 train acc :  0.7001030927835052     val loss :  0.9227601480435091 val acc :  0.6711910774410774\n",
            "epoch :  81   train loss :  0.8782942218459199 train acc :  0.6920038659793815     val loss :  0.9370399848527631 val acc :  0.6717171717171717\n",
            "epoch :  82   train loss :  0.8753051918574919 train acc :  0.6987532216494846     val loss :  0.9302125932014863 val acc :  0.6656407828282828\n",
            "epoch :  83   train loss :  0.8733760601203691 train acc :  0.6974291237113402     val loss :  0.9258493713419946 val acc :  0.6809501262626263\n",
            "epoch :  84   train loss :  0.871149881988309 train acc :  0.7011533505154639     val loss :  0.918838242695501 val acc :  0.6723747895622896\n",
            "epoch :  85   train loss :  0.8679078306208818 train acc :  0.7015045103092784     val loss :  0.9054687759395679 val acc :  0.6868160774410774\n",
            "epoch :  86   train loss :  0.8693423260183891 train acc :  0.7013659793814433     val loss :  0.9004904023441309 val acc :  0.6790035774410774\n",
            "epoch :  87   train loss :  0.8675402843598613 train acc :  0.7026417525773196     val loss :  0.8951802456313244 val acc :  0.694207702020202\n",
            "epoch :  88   train loss :  0.862737743650868 train acc :  0.7003286082474227     val loss :  0.9052039723879006 val acc :  0.6766098484848485\n",
            "epoch :  89   train loss :  0.8600184612776591 train acc :  0.7004284793814434     val loss :  0.8813344479801316 val acc :  0.6951809764309764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(90,110):\n",
        "    loss_ep = 0  # add batch loss in epoch\n",
        "    acc_ep = 0\n",
        "    for batch_idx, batch in enumerate(trainLoader):\n",
        "        optimizer.zero_grad()\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "       \n",
        "        loss = CrossEL(pred, label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step() \n",
        "\n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0] #acc/(batch*channels*4(augmented))\n",
        "        loss_ep += loss.item()\n",
        "        # print('acc:', acc)\n",
        "        acc_ep += acc\n",
        "\n",
        "    loss_tr.append((loss_ep)/len(trainLoader))\n",
        "    acc_tr.append((acc_ep)/len(trainLoader))        \n",
        "\n",
        "    loss_ep_val = 0\n",
        "    acc_ep_val = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(valLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_val += loss.item()\n",
        "        acc_ep_val += acc\n",
        "\n",
        "    loss_val.append((loss_ep_val)/len(valLoader))\n",
        "    acc_val.append((acc_ep_val)/len(valLoader))\n",
        "    print(\"epoch : \", epoch, \"  train loss : \", loss_tr[epoch], 'train acc : ', acc_tr[epoch], \"    val loss : \", loss_val[epoch], 'val acc : ', acc_val[epoch])\n",
        "    torch.save(model,'SleepEDF_Conv_30s_ep' + str(epoch)+'_.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHblO57_Rodc",
        "outputId": "1b0abee2-21b4-471c-c38c-6bbf2c6b2d5b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :  90   train loss :  0.8598942842106599 train acc :  0.7058666237113402     val loss :  0.9023787939248008 val acc :  0.6712962962962963\n",
            "epoch :  91   train loss :  0.8560641919295682 train acc :  0.7099903350515464     val loss :  0.9019892751531994 val acc :  0.6800820707070707\n",
            "epoch :  92   train loss :  0.8518199484299006 train acc :  0.7052899484536083     val loss :  0.8839498886704844 val acc :  0.6927872474747475\n",
            "epoch :  93   train loss :  0.8513056035566662 train acc :  0.7064787371134021     val loss :  0.898904694646697 val acc :  0.679871632996633\n",
            "epoch :  94   train loss :  0.8506347275608622 train acc :  0.7091784793814433     val loss :  0.9011855435582211 val acc :  0.685395622895623\n",
            "epoch :  95   train loss :  0.8483742607658467 train acc :  0.7083279639175258     val loss :  0.9194035969063798 val acc :  0.6912615740740741\n",
            "epoch :  96   train loss :  0.8470521723037885 train acc :  0.7099033505154639     val loss :  0.8954855407069213 val acc :  0.6936553030303031\n",
            "epoch :  97   train loss :  0.8448491174898791 train acc :  0.7077287371134021     val loss :  0.8868119632041069 val acc :  0.6842119107744108\n",
            "epoch :  98   train loss :  0.8429502989525743 train acc :  0.7099162371134021     val loss :  0.8876174341191047 val acc :  0.6910511363636364\n",
            "epoch :  99   train loss :  0.839427930424644 train acc :  0.710402706185567     val loss :  0.8725482559499197 val acc :  0.7032039141414141\n",
            "epoch :  100   train loss :  0.8390015627051429 train acc :  0.7132538659793815     val loss :  0.8768080239678966 val acc :  0.687684132996633\n",
            "epoch :  101   train loss :  0.8383061759817305 train acc :  0.7129155927835051     val loss :  0.8815413974341887 val acc :  0.6875789141414141\n",
            "epoch :  102   train loss :  0.8361957815871274 train acc :  0.7109664948453609     val loss :  0.865270219711098 val acc :  0.6970223063973064\n",
            "epoch :  103   train loss :  0.8326013409390418 train acc :  0.7113273195876288     val loss :  0.8878721199846029 val acc :  0.6921296296296297\n",
            "epoch :  104   train loss :  0.8312073440624391 train acc :  0.7152029639175258     val loss :  0.8915499376234703 val acc :  0.695496632996633\n",
            "epoch :  105   train loss :  0.8302808679197105 train acc :  0.7156282216494846     val loss :  0.8645555851873864 val acc :  0.6953914141414141\n",
            "epoch :  106   train loss :  0.8305869891277194 train acc :  0.7139046391752578     val loss :  0.8646056399458377 val acc :  0.6982060185185186\n",
            "epoch :  107   train loss :  0.8266361052617364 train acc :  0.7171778350515464     val loss :  0.8826835326422211 val acc :  0.6913667929292929\n",
            "epoch :  108   train loss :  0.8242121866311172 train acc :  0.7187532216494845     val loss :  0.894679814491751 val acc :  0.6862636784511785\n",
            "epoch :  109   train loss :  0.8246734352552718 train acc :  0.7141655927835052     val loss :  0.8939144509690977 val acc :  0.6907091750841752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "col = ['loss_tr','loss_val','acc_tr','acc_val']\n",
        "data = np.array([loss_tr,\n",
        "                 loss_val,\n",
        "                 acc_tr,\n",
        "                 acc_val])\n",
        "print(data.shape)\n",
        "data = np.transpose(data)\n",
        "df = pd.DataFrame(data = data, columns= col)\n",
        "df.to_excel('SleepEDF_conv10s.xlsx', index = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dLGi88mRVw5",
        "outputId": "eb40bb9a-b10e-4ea3-b14e-d6df02b5126e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 110)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 110"
      ],
      "metadata": {
        "id": "TjQgJ4OsFn5M"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "# plt.figure(figsize =(15, 10))\n",
        "plt.plot(range(epochs), loss_tr, color='red')\n",
        "plt.plot(range(epochs), loss_val, color='blue')\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.savefig('SleepEDF_conv10s_loss.png',bbox_inches = 'tight')\n",
        "plt.show() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ef2388bd-378f-4bcc-f0ad-741a3835b27e",
        "id": "aRBbSnNtQbkD"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df3hcZZ338fd3ZpJM87Npmv6gAdKCBUToDwIiVSk/9FLkghVBqKtQYeWCax8U1hWFVWF1fdZd2RV8dmUXQVDoQ5cHhEUFFbogICvQ1sIC5TcFAqVNQ5vfmcnMfJ8/zpk2aRtI00wmmfN5XddcmTnnzLnvM6f9nHvuuecec3dERCQ6YsWugIiIjC8Fv4hIxCj4RUQiRsEvIhIxCn4RkYhR8IuIRIyCX2QYZtZsZm5miRFsu9zMHtnb/YiMBwW/lAQz22BmaTObvtPyP4Wh21ycmolMPAp+KSWvAsvyD8zsMKCyeNURmZgU/FJKbgbOHvT4HODngzcwszoz+7mZtZnZa2b2TTOLheviZnaVmW0xs1eAT+3muTeY2UYze9PM/s7M4ntaSTPbx8zuNrN3zOwlM/vSoHVHmdlqM+s0s01m9s/h8qSZ3WJm7Wa2zcyeMLOZe1q2CCj4pbT8Eag1s0PCQD4LuGWnbf4PUAfMA44luFB8MVz3JeBkYBHQApy+03NvAjLAgeE2Hwf+YhT1XAm0AvuEZfxvMzs+XHcNcI271wIHALeFy88J670v0ABcAPSNomwRBb+UnHyr/2PAeuDN/IpBF4PL3L3L3TcA/wR8Idzks8DV7v6Gu78D/P2g584ETgIudvced98M/DDc34iZ2b7AEuDr7t7v7uuA69nxTmUAONDMprt7t7v/cdDyBuBAd8+6+xp379yTskXyFPxSam4GPgcsZ6duHmA6UAa8NmjZa8Cc8P4+wBs7rcvbP3zuxrCrZRvw78CMPazfPsA77t41TB3OA+YDz4XdOScPOq7fAivN7C0z+0czK9vDskUABb+UGHd/jeBD3pOAX+y0egtBy3n/Qcv2Y8e7go0EXSmD1+W9AaSA6e4+NbzVuvuhe1jFt4BpZlazuzq4+4vuvozggvIPwO1mVuXuA+7+t+7+fuAYgi6psxEZBQW/lKLzgOPdvWfwQnfPEvSZf8/Masxsf+Cv2PE5wG3Al82syczqgW8Meu5G4HfAP5lZrZnFzOwAMzt2Tyrm7m8AjwJ/H35ge3hY31sAzOzzZtbo7jlgW/i0nJkdZ2aHhd1VnQQXsNyelC2Sp+CXkuPuL7v76mFWXwT0AK8AjwD/F/hpuO4nBN0pTwJr2fUdw9lAOfAssBW4HZg9iiouA5oJWv93Ale4+/3huk8Az5hZN8EHvWe5ex8wKyyvk+Czi98TdP+I7DHTD7GIiESLWvwiIhGj4BcRiRgFv4hIxCj4RUQiZlJMEzt9+nRvbm4udjVERCaVNWvWbHH3xp2XT4rgb25uZvXq4UbniYjI7pjZa7tbrq4eEZGIUfCLiESMgl9EJGImRR+/iJSOgYEBWltb6e/vL3ZVSkYymaSpqYmyspFN2Fqw4DeznxLMILjZ3T+w07qvAlcBje6+pVB1EJGJp7W1lZqaGpqbmzGzYldn0nN32tvbaW1tZe7cuSN6TiG7em4imHBqiPCHKD4OvF7AskVkgurv76ehoUGhP0bMjIaGhj16B1Ww4Hf3h4B3drPqh8ClgGaHE4kohf7Y2tPXc1w/3DWzU4E33f3JEWx7fvij06vb2tpGVd4vfwnf//6onioiUrLGLfjNrBK4HPj2SLZ39+vcvcXdWxobd/ni2Yj85jdw1VWjeqqIlKj29nYWLlzIwoULmTVrFnPmzNn+OJ1Ov+tzV69ezZe//OVxqmnhjOeongOAucCT4duSJmCtmR3l7m8XosBEArLZQuxZRCarhoYG1q1bB8CVV15JdXU1f/3Xf719fSaTIZHYfTS2tLTQ0tIyLvUspHFr8bv7/7j7DHdvdvdmoBVYXKjQhyD4M5lC7V1ESsXy5cu54IIL+OAHP8ill17K448/zoc+9CEWLVrEMcccw/PPPw/Agw8+yMknnwwEF41zzz2XpUuXMm/ePH70ox8V8xD2SCGHc94KLAWmm1krwc/L3VCo8nZHwS8ywV18MYSt7zGzcCFcffUeP621tZVHH32UeDxOZ2cnDz/8MIlEgvvvv5/LL7+cO+64Y5fnPPfcczzwwAN0dXVx0EEHceGFF454LH0xFSz43X3Ze6xvLlTZeQp+ERmpM844g3g8DkBHRwfnnHMOL774ImbGwMDAbp/zqU99ioqKCioqKpgxYwabNm2iqalpPKs9KiX9zd188LuDRo+JTECjaJkXSlVV1fb73/rWtzjuuOO488472bBhA0uXLt3tcyoqKrbfj8fjZCZJS7Ok5+oJL97kcsWth4hMLh0dHcyZMweAm266qbiVKYCSDv78B/OT5CIsIhPEpZdeymWXXcaiRYsmTSt+T5j7xP8CbUtLi4/mh1j+8R/h61+Hnh6orCxAxURkj61fv55DDjmk2NUoObt7Xc1sjbvvMv5ULX4RkYhR8IuIRIyCX0QkYhT8IiIRo+AXEYmYkg7+/Dh+Bb+IyA4lHfz5Fr9m6BSRvOOOO47f/va3Q5ZdffXVXHjhhbvdfunSpeSHk5900kls27Ztl22uvPJKrnqPOeDvuusunn322e2Pv/3tb3P//ffvafXHRCSCXy1+EclbtmwZK1euHLJs5cqVLFv2rtOLAXDPPfcwderUUZW7c/B/5zvf4cQTTxzVvvaWgl9EIuX000/n17/+9fYfXdmwYQNvvfUWt956Ky0tLRx66KFcccUVu31uc3MzW7ZsAeB73/se8+fP58Mf/vD2aZsBfvKTn3DkkUeyYMECPvOZz9Db28ujjz7K3Xffzde+9jUWLlzIyy+/zPLly7n99tsBWLVqFYsWLeKwww7j3HPPJZVKbS/viiuuYPHixRx22GE899xzY/IalPwkbaDgF5moijEr87Rp0zjqqKO49957OfXUU1m5ciWf/exnufzyy5k2bRrZbJYTTjiBp556isMPP3y3+1izZg0rV65k3bp1ZDIZFi9ezBFHHAHAaaedxpe+9CUAvvnNb3LDDTdw0UUXccopp3DyySdz+umnD9lXf38/y5cvZ9WqVcyfP5+zzz6ba6+9losvvhiA6dOns3btWn784x9z1VVXcf311+/1a6QWv4hEzuDunnw3z2233cbixYtZtGgRzzzzzJBumZ09/PDDfPrTn6ayspLa2lpOOeWU7euefvppPvKRj3DYYYexYsUKnnnmmXety/PPP8/cuXOZP38+AOeccw4PPfTQ9vWnnXYaAEcccQQbNmwY7SEPoRa/iBRNsWZlPvXUU7nkkktYu3Ytvb29TJs2jauuuoonnniC+vp6li9fTn9//6j2vXz5cu666y4WLFjATTfdxIMPPrhXdc1P/TyW0z6XdItfwzlFZHeqq6s57rjjOPfcc1m2bBmdnZ1UVVVRV1fHpk2buPfee9/1+R/96Ee566676Ovro6uri1/+8pfb13V1dTF79mwGBgZYsWLF9uU1NTV0dXXtsq+DDjqIDRs28NJLLwFw8803c+yxx47Rke5eSQe/WvwiMpxly5bx5JNPsmzZMhYsWMCiRYs4+OCD+dznPseSJUve9bmLFy/mzDPPZMGCBXzyk5/kyCOP3L7uu9/9Lh/84AdZsmQJBx988PblZ511Fj/4wQ9YtGgRL7/88vblyWSSG2+8kTPOOIPDDjuMWCzGBRdcMPYHPEhJT8v8yCPwkY/AffdBkUZNichONC1zYWha5pBa/CIiu1Lwi4hETMGC38x+amabzezpQct+YGbPmdlTZnanmY3uK3AjpOAXmZgmQxfzZLKnr2chW/w3AZ/Yadl9wAfc/XDgBeCyApav4BeZgJLJJO3t7Qr/MeLutLe3k0wmR/ycgo3jd/eHzKx5p2W/G/Twj8DQr7CNMQW/yMTT1NREa2srbW1txa5KyUgmkzQ1NY14+2J+getc4D+GW2lm5wPnA+y3336jKkDj+EUmnrKyMubOnVvsakRaUT7cNbO/ATLAiuG2cffr3L3F3VsaGxtHVY6mZRYR2dW4t/jNbDlwMnCCF7iTT109IiK7GtfgN7NPAJcCx7p7b6HLU/CLiOyqkMM5bwX+GzjIzFrN7DzgX4Aa4D4zW2dm/1ao8kHBLyKyO4Uc1bO7n7O5oVDl7Y6CX0RkV/rmrohIxJR08Gs4p4jIrko6+DWcU0RkV5EIfrX4RUR2KOngj8XATMEvIjJYSQc/BK1+Bb+IyA4KfhGRiFHwi4hEjIJfRCRiSj7443EFv4jIYCUf/ImExvGLiAwWieBXi19EZAcFv4hIxCj4RUQiRsEvIhIxCn4RkYgp+eDXcE4RkaFKPvg1nFNEZKhIBL9a/CIiOyj4RUQiRsEvIhIxBQt+M/upmW02s6cHLZtmZveZ2Yvh3/pClZ+n4BcRGaqQLf6bgE/stOwbwCp3fx+wKnxcUAp+EZGhChb87v4Q8M5Oi08Ffhbe/xnwZ4UqP0/BLyIy1Hj38c90943h/beBmYUuUOP4RUSGKtqHu+7ugA+33szON7PVZra6ra1t1OVoHL+IyFDjHfybzGw2QPh383Abuvt17t7i7i2NjY2jLlBdPSIiQ4138N8NnBPePwf4z0IXqOAXERmqkMM5bwX+GzjIzFrN7Dzg+8DHzOxF4MTwcUEp+EVEhkoUasfuvmyYVScUqszdUfCLiAylb+6KiERMyQe/hnOKiAxV8sGv4ZwiIkNFIvjV4hcR2UHBLyISMQp+EZGIUfCLiERMJILfHXK5YtdERGRiiETwg1r9IiJ5JR/88XjwV0M6RUQCJR/8avGLiAyl4BcRiRgFv4hIxCj4RUQiRsEvIhIxCn4RkYgp+eDPD+dU8IuIBEo++PMtfo3jFxEJRCb41eIXEQko+EVEIkbBLyISMQp+EZGIKUrwm9klZvaMmT1tZreaWbJQZSn4RUSGGvfgN7M5wJeBFnf/ABAHzipUeQp+EZGhitXVkwCmmFkCqATeKlRBmpZZRGSocQ9+d38TuAp4HdgIdLj773bezszON7PVZra6ra1t1OWpxS8iMlQxunrqgVOBucA+QJWZfX7n7dz9OndvcfeWxsbGUZen4BcRGaoYXT0nAq+6e5u7DwC/AI4pVGEKfhGRoUYU/Gb2FTOrtcANZrbWzD4+yjJfB442s0ozM+AEYP0o9/WeFPwiIkONtMV/rrt3Ah8H6oEvAN8fTYHu/hhwO7AW+J+wDteNZl8joeAXERkqMcLtLPx7EnCzuz8TttZHxd2vAK4Y7fP3hIJfRGSokbb415jZ7wiC/7dmVgPkCletsaPhnCIiQ420xX8esBB4xd17zWwa8MXCVWvsqMUvIjLUSFv8HwKed/dt4dDLbwIdhavW2FHwi4gMNdLgvxboNbMFwFeBl4GfF6xWY0jBLyIy1EiDP+PuTvDFq39x938FagpXrbGj4BcRGWqkffxdZnYZwTDOj5hZDCgrXLXGjoJfRGSokbb4zwRSBOP53waagB8UrFZjSMEvIjLUiII/DPsVQJ2ZnQz0u7v6+EVEJqGRTtnwWeBx4Azgs8BjZnZ6ISs2VjSOX0RkqJH28f8NcKS7bwYws0bgfoKpFya0WHhpU4tfRCQw0uCP5UM/1M5k+L3eO+/EnnySROJKBb+ISGikwf8bM/stcGv4+EzgnsJUaQw98ACsWKHgFxEZZETB7+5fM7PPAEvCRde5+52Fq9YYqayE3l4SCXX1iIjkjbTFj7vfAdxRwLqMvcpK6O8nMdXJZEY9maiISEl51+A3sy7Ad7cKcHevLUitxkplJQCJuFr8IiJ57xr87j4ppmUYVhj88ZiTzarFLyICk2Fkzt7Y3uLPqcUvIhKKRvDHFPwiInkKfhGRiFHwi4hETDSC37IKfhGRUFGC38ymmtntZvacma03sw8VpCAFv4jILkb8Ba4xdg3wG3c/3czKgcqClDIo+DU7p4hIYNyD38zqgI8CywHcPQ2kC1JYfhy/q8UvIpJXjK6euUAbcKOZ/cnMrjezqp03MrPzzWy1ma1ua2sbXUnbW/wZBb+ISKgYwZ8AFgPXuvsioAf4xs4buft17t7i7i2NjY2jKykf/K7gFxHJK0bwtwKt7v5Y+Ph2ggvB2Csrg3icBAMKfhGR0LgHf/j7vW+Y2UHhohOAZwtSmBlUVqrFLyIySLFG9VwErAhH9LwCfLFgJVVWkvC0gl9EJFSU4Hf3dUDLuBRWWUliQF09IiJ5pf3NXYDKSuK5AY3jFxEJRSL4Ezl19YiI5Cn4RUQiJhrBn00p+EVEQgp+EZGIiUjw9yv4RURC0Qj+jFr8IiJ5EQn+fg3nFBEJRSL445kUmYwXuyYiIhNCJIJfUzaIiOwQjeBHk7SJiOQp+EVEIiYywZ/LGblcsSsjIlJ8kQl+QCN7RESIWPCru0dEJCLBHydo6qvFLyISkeBXi19EZAcFv4hIxCj4RUQiRsEvIhIxCn4RkYgpWvCbWdzM/mRmvypoQVOmKPhFRAYpZov/K8D6gpcSj5NIGKDhnCIiUKTgN7Mm4FPA9eNRXrwiAajFLyICxWvxXw1cCgw7e46ZnW9mq81sdVtb214VlqiIAwp+EREoQvCb2cnAZndf827buft17t7i7i2NjY17VaaCX0Rkh2K0+JcAp5jZBmAlcLyZ3VLIAhNJdfWIiOSNe/C7+2Xu3uTuzcBZwH+5++cLWaaCX0Rkh9Ifx4+CX0RksEQxC3f3B4EHC11OYkoZoOGcIiIQkRZ/fEo5oBa/iAhEJPjzLX4Fv4hIVIK/UsEvIpIXjeBXV4+IyHbRCP6qCgAy6WG/KCwiEhnRCP7KsMXfN1DkmoiIFF80gj/f4u9NF7kmIiLFF6ngz/Yp+EVEIhH88aokoBa/iAhEJPgT1WHwq49fRETBLyISNdEK/n4Fv4hINIK/ZgoAmT59g0tEJFrB36/gFxGJVPBnUwp+EZFIBH+suhKATEoT8ouIRCL4raqSOBkyavGLiEQj+KmoIEGGTEqTtImIRCP4zcLgV1ePiEg0gh9IWFbTMouIoOAXEYmccQ9+M9vXzB4ws2fN7Bkz+8p4lJuwHNkBBb+ISKIIZWaAr7r7WjOrAdaY2X3u/mwhC03EcmR6UoUsQkRkUhj3Fr+7b3T3teH9LmA9MKfQ5cYry8ls2gKvvlrookREJrSi9vGbWTOwCHhsN+vON7PVZra6ra1tr8tK1FWToQxuuGGv9yUiMpkVLfjNrBq4A7jY3Tt3Xu/u17l7i7u3NDY27nV5iWSCzD77wo03QkZf5BKR6CpK8JtZGUHor3D3X4xHmYkEZPY/EN56C+65ZzyKFBGZkIoxqseAG4D17v7P41VuIgGZxtkwaxb85CfjVayIyIRTjBb/EuALwPFmti68nVToQhMJ6OiK4cu/GLT4W1sLXaSIyIRUjFE9j7i7ufvh7r4wvBW87+XII+GBB+DYVd/iqdyhsHx50O0jIhIxkfnm7o9/HPTwPPvKFBbZOi568DNsPfTDcNtt4F7s6omIjJvIBH8sBn/xF/DCC3DBhTF+7Bcwv3sN1515P9sOXQLXXAPt7cWupohIwZlPgtZuS0uLr169ekz3uW4dXPS/nEf+YAAcwrO0sIYDGzuYNz/BIUdWs+C4aSTePx+am4MPCUREJhEzW+PuLbssj2rwQ9DD8+CD8Ic/wGP3dfCnP8GbXXXb11fTxRL+QLO9Try2kkRdNbNnOQfMyzHvkCQHHDGVqR9ogjlzdGEQkQlHwT9C/f2wYQM8+XAnD93bw8OPV7BpaxnZjDOQidGZqx6yfQNbmMurzKzYxoyqXmprHauugqpqZs+GRYdnWXBUBY0H1GLT6qG+HsrKxuVYRCTaFPxjpKsLXnmmj5efeIeXn+rhpeezvPZWgs3bytncU0VXugLccYcuarc/r4J+ZrCZBtpJxJxYWYyyMqhNDlBXlaG2OkdNDdTUxaitj1HbUE7t9HIq68uprE+SnJokXlNJrGoKVTUx5s6FZLKIL4SITHjDBb/6J/ZQTQ0sOHoKC45+j3nlMhnan3+bJx/p4qm1GTa+lWNzW4wtW6vIpjJ4eoB0KkdbbxUvdyTpyFTR5dX0UjWiehg5mhKbmF7eAbE4xGIky7JUVwxQnczQUJNm+tQBamuNruwUtqUr6cslSVbGSFbHSVbFSVYGf6c2xJk+O8H02eXMmB1nxgyorAze+bzyCqRS8L73BbdMBp57LviQPP961NdDS4suRCKThYK/UBIJGg6dxfGHzuL4kT4nlyO7dRvdb3bQ+WYXnRt76N2aondbmr6ONLmePnK9/XRszfHy5hpebK9nW18Flsng2SypvjjdnUneylbyTq6RNhrJUEaCAerZSpJ++imnjymkqGCA8j06JCOHDzMQrLIszQkHvs7R8zbT1lfN6511ZEnQvE+K5jkZkpUxutNl9KTKiJXFKUvGsbI4WzrKeXtLgr50nOZ5MeYdGGPffYOLSX09mEFfX9AFV1MDDQ3B3y1bYNOmYPl++wW3fA9aLhc8z2yPDm+32tth61Y44ICx2Z/IRKDgn0hiMeINU6lrmErd4Xu5L3e8P0X/lg6S3of19UJPD3R3B/1VfX1ke1P0d6bZttVpf8doeydO2ztxNm8to7vH2L/ibeaVt1Ix0M0LWxt5vms2ZZl+Dom/wHx/nkSqh66+BBv76/ndwMf41fqT+eX6Y6iim/14nThZVtFMNzXDVjNJHzPZSAUpfs1+9DNlVIcbsxxViTT92QQDueCftZlTFssyo6aPOfV9zKpPUZnMUVERXCQsESOWiDPgcXrTCXpTCfoG4vQPxOnui/PqGwm2bgsudDNnOieeAAsWGgMDwQXHHeLx4HP9ujqYNg2mT4d99oGmJpg6FQYGoLc32D6Xg2w2WJZKBbd4nO31gWCbWAxqa4N9asyAFIL6+GXvuUMmg6fSdLenqI73Yf190NeH9/XT/vYAA30ZqhP9VMb6IZ0i3ZMh15+m0vqwTJCEue5eNm6Os7G9nK095WztLsMzWaZ4LxXZXrr6y2jvq6S7P0ED7cz0tykf6OG1/pm8mt6H7swUkt5HRa4Py2XIeJw05WxiJq00sYmZ9JMM3+2U4RiOESdLFT1MoY9Kerf/3Z/XOJCXqKab33Ms9/Ex2pix/bBjZMkRH/Zlea/1IzGlbICqigxVySwN1WnmNPQxsy7F252VPL+xhjfbkzQ1pjigKcWshgz9A3H60nG6euNs646zrSvO1NocB+yf5YC5OWbNNhpnxairj9PbZ3R2Gb19MJCLM5CJkUxCY2NwAevpCb7cvnkzVFcHy2trg3ZDR0fwTiyRCG5lZcGtvHzHMrPg3VJ7O3R2Bl2BlZXB3/Ly4JZfVlkZXCinTQve6U2ZEqyP7/TyuQcX0d2ty+V2fBczFnv3d2i5XPCuceNG2LYtGJi3337BfkuJPtyV6Mlmg5To79/R7M5kgiZ3vtnd3w/p9I5l6XSwLJUKts3lgr+pFLm+FN1dTtL7KBvoxXJZPJMlk4GO7jjtnWW0dSV5q6eON/um0Z6qZgq9VOW6qcj1Es+miWXSlGX6qfA+KrJ9ZHOQzpWRziWwXJZYdoCsGx3U0UEdndTSQxU9VLGF6bzJHDYxk5lsYj4v0EQrbzKHlziQzczYftGqood6tlJHB+8wjZc4kA00ky3Sm/yEZcj4npediGWpiGdIJrI40JmqIJOLUxbPsn9DN/tN66Gjv5w33qlic+eOd4vxWI6Z9WlmT0tTVuZs6SijbVs5fekY2ZyRze56VYjFnBnTMtRU5aiudMrLnZwbYNTUOI3Tg65GxxjIQCplpNJGegBiMWPGDJg5y0hnYmx4zXj99eAN9sBAcEHad9/gc7L999/xTq63N7j4bNwYXIi2bg0uRNnsjgvXLbfA0qV7/pqDgl9k8sjlggvQ4AtTKhWkQb6/KL8svzyTCW75+9lskDbZ7Pb+pWz/AFs7YrRtTdDRFaO6PE1NRZqqRIqyXIpEpp++PmjrqmBLV5Jq62Gf8i1Mt3Z60mW09VXTmaqgNtZNXbybKfQFw5zTzkDGGMgY6QEjO5Aj058hl84wNdZJQ3wbU6yfbMaDdySZMgYyRiqbIJVN0JtL0pOpYFu2hvbsVLZ6XfDOzJKkvIIU5aSoAKCODqrppoM6XmUur7MfdXSwL2+wD2+RIPitjT6m8Daz2MhsMiRopI3pbKGSXuJkSZChkTZms5E6OniTObzCPN5iH3qooptqUlQQI/id7k5q2cJ02mnAcMpJU8YAFaSoIEWGBG000k4DcbLsyxvsZ63UxTopIwNmvOb78kL2QDoHjfYDqLetzI630Zh4h/p4F3XxbhKWBcCBS66ZywfOOWJU/5Q0qkdksojFgj6QZDLo6B8jcWB6eBtO1TDra8Pb3pZfFd7elfuu/TTuQz8cGXyh89nAwTvenaXTQ9/FZTLBNj4r2E/+lr+I5i+ogy+U2R7Idu7YJtO6Y3+53I465RvO4XaZdA7LZohn0zu2ze/DHc85XekKnOD4KixN0vuC+g6+cMOO12DBZXv5yu9KwS8iE8vuOufNdnwwUDP8YIFie69ANfb+AjoWIjNJm4iIBBT8IiIRo+AXEYkYBb+ISMQo+EVEIkbBLyISMQp+EZGIUfCLiETMpJiywczagNdG+fTpwJYxrM5Eo+Ob3HR8k9dkOLb93b1x54WTIvj3hpmt3t1cFaVCxze56fgmr8l8bOrqERGJGAW/iEjERCH4ryt2BQpMxze56fgmr0l7bCXfxy8iIkNFocUvIiKDKPhFRCKmpIPfzD5hZs+b2Utm9o1i12dvmNm+ZvaAmT1rZs+Y2VfC5dPM7D4zezH8W1/suu4NM4ub2Z/M7Ffh47lm9lh4Dv/DzCbtz2Gb2VQzu93MnjOz9Wb2oVI6f2Z2Sfhv82kzu9XMkpP5/JnZT81ss5k9PWjZbs+XBX4UHudTZra4eDV/byUb/GYWB/4V+CTwfmCZmb2/uLXaKxngq+7+fuBo4C/D4/kGsMrd3wesCh9PZl8B1g96/A/AD939QGArcF5RamjNKrcAAASqSURBVDU2rgF+4+4HAwsIjrMkzp+ZzQG+DLS4+wcIfmnxLCb3+bsJ+MROy4Y7X58E3hfezgeuHac6jkrJBj9wFPCSu7/i7mlgJXBqkes0au6+0d3Xhve7CEJjDsEx/Szc7GfAnxWnhnvPzJqATwHXh48NOB64Pdxk0h6fmdUBHwVuAHD3tLtvo4TOH8EvD04xswRQCWxkEp8/d38IeGenxcOdr1OBn3vgj8BUM5s9PjXdc6Uc/HOANwY9bg2XTXpm1gwsAh4DZrr7xnDV28DMIlVrLFwNXAqEv2ZNA7DN3cNfn57U53Au0AbcGHZlXW9mVZTI+XP3N4GrgNcJAr8DWEPpnL+84c7XpMqbUg7+kmRm1cAdwMXu3jl4nQdjcyfl+FwzOxnY7O5ril2XAkkAi4Fr3X0R0MNO3TqT/PzVE7R65wL7AFXs2k1SUibz+Srl4H8T2HfQ46Zw2aRlZmUEob/C3X8RLt6Uf0sZ/t1crPrtpSXAKWa2gaBb7niCPvGpYdcBTO5z2Aq0uvtj4ePbCS4EpXL+TgRedfc2dx8AfkFwTkvl/OUNd74mVd6UcvA/AbwvHFVQTvBB091FrtOohf3dNwDr3f2fB626GzgnvH8O8J/jXbex4O6XuXuTuzcTnKv/cvc/Bx4ATg83m8zH9zbwhpkdFC46AXiWEjl/BF08R5tZZfhvNX98JXH+BhnufN0NnB2O7jka6BjUJTTxuHvJ3oCTgBeAl4G/KXZ99vJYPkzwtvIpYF14O4mgH3wV8CJwPzCt2HUdg2NdCvwqvD8PeBx4Cfh/QEWx67cXx7UQWB2ew7uA+lI6f8DfAs8BTwM3AxWT+fwBtxJ8XjFA8I7tvOHOF2AEowhfBv6HYHRT0Y9huJumbBARiZhS7uoREZHdUPCLiESMgl9EJGIU/CIiEaPgFxGJGAW/SAGY2dL8DKMiE42CX0QkYhT8Emlm9nkze9zM1pnZv4e/B9BtZj8M55ZfZWaN4bYLzeyP4Xzrdw6ai/1AM7vfzJ40s7VmdkC4++pB8++vCL/Ripl9P/xdhafM7KoiHbpEmIJfIsvMDgHOBJa4+0IgC/w5wQRjq939UOD3wBXhU34OfN3dDyf4dmZ++QrgX919AXAMwbc9IZhB9WKC34OYBywxswbg08Ch4X7+rrBHKbIrBb9E2QnAEcATZrYufDyPYFro/wi3uQX4cDif/lR3/324/GfAR82sBpjj7ncCuHu/u/eG2zzu7q3uniOYYqOZYLrifuAGMzsNyG8rMm4U/BJlBvzM3ReGt4Pc/crdbDfaeU1Sg+5ngYQHc9MfRTA758nAb0a5b5FRU/BLlK0CTjezGbD991T3J/h/kZ9R8nPAI+7eAWw1s4+Ey78A/N6DX0NrNbM/C/dRYWaVwxUY/p5CnbvfA1xC8BOMIuMq8d6biJQmd3/WzL4J/M7MYgSzMP4lwY+kHBWu20zwOQAE0/D+WxjsrwBfDJd/Afh3M/tOuI8z3qXYGuA/zSxJ8I7jr8b4sETek2bnFNmJmXW7e3Wx6yFSKOrqERGJGLX4RUQiRi1+EZGIUfCLiESMgl9EJGIU/CIiEaPgFxGJmP8PH11D0vKBdNAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize =(15, 10))\n",
        "plt.plot(range(epochs), acc_tr, color='red')\n",
        "plt.plot(range(epochs), acc_val, color='blue')\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.savefig('SleepEDF_conv10s_accuracy.png',bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "56b95e13-551c-449d-b79e-ec811198c1d6",
        "id": "sN6ihZN4QbkE"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/SleepEDF_Conv_30s_ep99_.pt').to(device)"
      ],
      "metadata": {
        "id": "V4ccVb0ETOsY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_ep_test = 0\n",
        "acc_ep_test = 0\n",
        "for batch_idx, batch in enumerate(testLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "        loss_ep_test += loss.item()\n",
        "        acc_ep_test += acc\n",
        "\n",
        "acc_ep_test = acc_ep_test/len(testLoader)\n",
        "loss_ep_test = loss_ep_test/len(testLoader)\n",
        "\n",
        "print(acc_ep_test)\n",
        "print(loss_ep_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GMhsbW-tHab",
        "outputId": "9b6dc029-f953-4063-a6ff-250d851c53b6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7394518097643098\n",
            "0.8012365100297059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = []\n",
        "predicted_list = []\n",
        "for batch_idx, batch in enumerate(testLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding(data) \n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        label_list.extend(label.cpu().detach().numpy())\n",
        "        predicted_list.extend(predicted.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "JXgSul4kDlpz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix   \n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_matrix = confusion_matrix(label_list, predicted_list)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "c-y5mOdw49Y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cb84c6-05f4-4b72-c76a-e4d14c3b3cc7"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 94   1  18   0   2  12]\n",
            " [ 25   2  32   0   0  29]\n",
            " [  9   1 468   7   5  23]\n",
            " [  0   0  39  26  11   0]\n",
            " [  0   0  19  18  45   0]\n",
            " [  8   1  20   0   0 142]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title = 'confusion matrix'\n",
        "cmap=plt.cm.Greens\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)  # , cmap=plt.cm.Greens\n",
        "plt.title(title, size=12)\n",
        "plt.colorbar(fraction=0.05, pad=0.05)\n",
        "tick_marks = np.arange(6, 6)\n",
        "plt.yticks(np.arange(6), ('W','N1','N2','N3','N4','R'))\n",
        "plt.xticks(np.arange(6), ('W','N1','N2','N3','N4','R'))\n",
        "\n",
        "\n",
        "fmt = 'd' \n",
        "thresh = 1\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
        "                 ha=\"center\", va=\"center\", color=\"black\" if conf_matrix[i, j] > thresh else \"black\")  #horizontalalignment\n",
        "plt.savefig('SleepEDF_conv10s_confusion matrix.png',bbox_inches = 'tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "l1JMllMxSB7b",
        "outputId": "a4f95d52-944e-4323-8856-0b278aa6a334"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}