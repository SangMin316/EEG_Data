{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLhzhFpgNwjT",
        "outputId": "28b02f07-3192-46a6-d0c6-22ff41cc4b2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDWt74XAgpLK",
        "outputId": "1fa42426-5035-4f4d-eb3a-151d789b1a68"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.2.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l4YUYhC7E4Ck"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pickle\n",
        "import mne\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Sleepedf_dataset(Dataset):\n",
        "    def __init__(self, files, seq_len, SSL = bool):\n",
        "        self.files = files\n",
        "        self.sequence_length = seq_len\n",
        "        self.SSL = SSL\n",
        "        # sample을 split해줬을 때 몇개로 split되는지 누적해서 저장, i번째 data를 찾을 때 data_adress 각 값이 기준이 됨\n",
        "        data_adress = [0]\n",
        "        ad = 0\n",
        "        max_value = 0.\n",
        "        min_value = 0.\n",
        "\n",
        "        for i in range(len(self.files)):\n",
        "            sample = np.load(files[i])['x']\n",
        "            c,t = sample.shape\n",
        "            t = int(t/self.sequence_length) \n",
        "            ad += t\n",
        "            data_adress.append(ad)\n",
        "            temp_max = sample.max()\n",
        "            temp_min = sample.min()\n",
        "            max_value = np.max([max_value, temp_max])\n",
        "            min_value = np.min([min_value, temp_min])\n",
        "        \n",
        "        self.data_adress = data_adress\n",
        "        self.max_value = max_value\n",
        "        self.min_value = min_value\n",
        "\n",
        "    def preprocessing(self, data):\n",
        "        data_max = np.max(data,axis = 2, keepdims=True) # max value of each channels\n",
        "        data_min = np.min(data,axis = 2, keepdims=True) # shape = b,c\n",
        "        b,c,t = data.shape\n",
        "        \n",
        "        return data/data_max*np.ones((b,c,t)) - (data_max - data_min)*np.ones((b,c,t))/(self.max_value - self.min_value)\n",
        "\n",
        "\n",
        "    def split_data(self, data):\n",
        "        L = self.sequence_length\n",
        "        channels, length = data.shape\n",
        "        a = L*int(length/L)\n",
        "        \n",
        "        if length == a:\n",
        "            data = np.reshape(data,(int(length/L),channels,L))\n",
        "        \n",
        "        else:\n",
        "            data = data[:,:a]\n",
        "            data = np.reshape(data,(int(a/L),channels,L))\n",
        "        return data\n",
        "\n",
        "    def one_hot_encoding(self,y):\n",
        "        if y == 'W':\n",
        "          y = np.array([1,0,0,0,0])\n",
        "        elif y == '1':\n",
        "          y = np.array([0,1,0,0,0])\n",
        "        elif y == '2':\n",
        "          y = np.array([0,0,1,0,0])   \n",
        "        elif y == '3':\n",
        "          y = np.array([0,0,0,1,0])\n",
        "        elif y == '4':\n",
        "          y = np.array([0,0,0,0,1])      \n",
        "        return y  \n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        for i in range(len(self.data_adress)):\n",
        "            if index < self.data_adress[i]:\n",
        "                break\n",
        "          \n",
        "        sample = np.load(self.files[i-1])  \n",
        "        y = self.one_hot_encoding(sample['y'])\n",
        "        sample = self.split_data(sample['x'])\n",
        "        sample = self.preprocessing(sample)\n",
        "\n",
        "        if self.SSL:\n",
        "          return sample[index - self.data_adress[i-1],:,:]\n",
        "        else:\n",
        "          return { 'x' : torch.tensor(sample[index - self.data_adress[i-1],:,:]), \n",
        "                   'y' : torch.tensor(y)\n",
        "                   }\n",
        "\n",
        "          \n",
        "          \n",
        "    def __len__(self):\n",
        "        return self.data_adress[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MASS_dataset(Dataset):\n",
        "    def __init__(self, files):\n",
        "        self.files = files\n",
        "        max_value = 0.\n",
        "        min_value = 0.\n",
        "\n",
        "        for i in range(len(self.files)):\n",
        "            sample = np.load(files[i])['x']\n",
        "            temp_max = sample.max()\n",
        "            temp_min = sample.min()\n",
        "            max_value = np.max([max_value, temp_max])\n",
        "            min_value = np.min([min_value, temp_min])\n",
        "        \n",
        "\n",
        "        self.max_value = max_value\n",
        "        self.min_value = min_value\n",
        "\n",
        "    def preprocessing(self, data):\n",
        "        data_max = np.max(data,axis = 1, keepdims=True) # max value of each channels\n",
        "        data_min = np.min(data,axis = 1, keepdims=True) # shape = c,t\n",
        "        c,t = data.shape\n",
        "        \n",
        "        return data/data_max*np.ones((c,t)) - (data_max - data_min)*np.ones((c,t))/(self.max_value - self.min_value)\n",
        "\n",
        "\n",
        "    def split_data(self, data):\n",
        "        L = self.sequence_length\n",
        "        channels, length = data.shape\n",
        "        a = L*int(length/L)\n",
        "        \n",
        "        if length == a:\n",
        "            data = np.reshape(data,(int(length/L),channels,L))\n",
        "        \n",
        "        else:\n",
        "            data = data[:,:a]\n",
        "            data = np.reshape(data,(int(a/L),channels,L))\n",
        "        return data\n",
        "\n",
        "    def one_hot_encoding(self,y):\n",
        "        if y == 'Sleep stage W':\n",
        "          y = np.array([1,0,0,0,0])\n",
        "        elif y == 'Sleep stage 1':\n",
        "          y = np.array([0,1,0,0,0])\n",
        "        elif y == 'Sleep stage 2':\n",
        "          y = np.array([0,0,1,0,0])   \n",
        "        elif y == 'Sleep stage 3':\n",
        "          y = np.array([0,0,0,1,0])\n",
        "        elif y == 'Sleep stage R':\n",
        "          y = np.array([0,0,0,0,1])      \n",
        "        return y  \n",
        "\n",
        "\n",
        "    def __getitem__(self, index):          \n",
        "        sample = np.load(self.files[index])  \n",
        "        y = self.one_hot_encoding(sample['y'])\n",
        "        sample = self.preprocessing(sample['x'])\n",
        "        return { 'x' : torch.tensor(sample), \n",
        "                 'y' : torch.tensor(y)\n",
        "                   }\n",
        "          \n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "metadata": {
        "id": "x59y8HGhPIQB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk5UWrymWlfa",
        "outputId": "949bee47-6765-443e-f185-25b68446c5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "4683\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "SS1_list = glob.glob('/content/drive/MyDrive/EEG_data/MASS/SS1/Preprocessed_EEG/17channels/**')\n",
        "print(len(SS1_list))\n",
        "MASS_list = []\n",
        "for i in range(len(SS1_list)):\n",
        "    MASS_list.extend(glob.glob(SS1_list[i]+'/**'))\n",
        "print(len(MASS_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(MASS_list, test_size=0.2)#, shuffle=True, random_state=34), #stratify=target\n",
        "train, val = train_test_split(train, test_size= 0.25)#, shu"
      ],
      "metadata": {
        "id": "8hZHBofhD_ti"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGvyMclMw_Vk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MASS_dataset(train)\n",
        "val_dataset = MASS_dataset(val)\n",
        "test_dataset = MASS_dataset(test)"
      ],
      "metadata": {
        "id": "eY3pJkIbWoFC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install separableconv-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "834tX7V_uCyj",
        "outputId": "14fd8a06-9287-4f04-e73d-00d304e58e20"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: separableconv-torch in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from separableconv-torch) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from separableconv-torch) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from separableconv-torch) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->separableconv-torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->separableconv-torch) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->separableconv-torch) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->separableconv-torch) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from mne.filter import filter_data, notch_filter\n",
        "import matplotlib.pyplot as plt\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, fs, encode_info):\n",
        "        super(Encoder, self).__init__()\n",
        "        #spectral layer means spectral convolution\n",
        "        #self.bac_layer is consist of several SeparableConv2d, which plays the role of temporal separable convolution\n",
        "        #convolution layer are initiated by xavier_uniform initization\n",
        "        #Input are Normalized by self.bn(=torch.nn.BatchNorm2d)\n",
        "        #[batch, electrode, length] -> [batch, electrode, Feature]\n",
        "        self.fs = fs\n",
        "        self.elu = nn.ELU()\n",
        "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.bn = nn.BatchNorm1d(1)\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "        self.spectral_layer = nn.Conv1d(1, 10, int(self.fs/2), padding=\"same\")\n",
        "\n",
        "        # self.bac_layer = nn.Sequential()\n",
        "        # for i, arg in enumerate(encode_info):\n",
        "        #     input_dim, output_dim, kernel_size = arg\n",
        "        #     self.bac_layer.add_module(\"temporal_conv_\"+str(i),\n",
        "        #                           nn.Conv1d(input_dim, output_dim, kernel_size, padding = 'same'))\n",
        "        #     self.bac_layer.add_module(\"ELU\",nn.ELU()) \n",
        "        \n",
        "        self.conv1t = nn.Conv1d(10,16, 30, padding ='same')\n",
        "        self.conv2t = nn.Conv1d(16,32, 15, padding ='same')\n",
        "        self.conv3t = nn.Conv1d(32,64, 5, padding ='same')\n",
        "        \n",
        "        # torch.nn.init.xavier_uniform_(self.spectral_layer.weight)\n",
        "        #self.bac_layer.apply(weight_init_xavier_uniform)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.spectral_layer(x))\n",
        "        x = self.activation(self.conv1t(x))\n",
        "        x = self.activation(self.conv2t(x))\n",
        "        x = self.activation(self.conv3t(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "#Linear layer for SSL classification\n",
        "class Head_NN(nn.Module):\n",
        "    def __init__(self, length):\n",
        "        super(Head_NN, self).__init__()\n",
        "        self.length = length\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 5)\n",
        "        )\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "        # self.layer.apply(weight_init_xavier_uniform)\n",
        "        self.bn = nn.BatchNorm1d(64)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.mean(x, axis = 2) # Global average pooling into temporal dimension\n",
        "        x = self.layer(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "class StoppedBandPathway(nn.Module):\n",
        "    def __init__(self, fs, Unsupervise, encode_info, bands):\n",
        "        super(StoppedBandPathway, self).__init__()\n",
        "        self.encoder = Encoder(fs, encode_info)\n",
        "        self.pretrain = Head_NN(2000)\n",
        "        self.Unsupervise = Unsupervise\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.pretrain(x)\n",
        "        return x\n",
        "\n",
        "    def getRep(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "glstHzJT3A0Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class feature_extractor3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(feature_extractor3,self).__init__()\n",
        "        \n",
        "        self.channels = 1 # we use only single channel \n",
        "        \n",
        "        # Activation functions\n",
        "        self.activation = nn.LeakyReLU()\n",
        "        # self.bn = nn.BatchNorm1d(1)\n",
        "\n",
        "        # self.conv2t = nns.SeparableConv1d(16,32,10,padding ='same') (in_channels, out_channels, kernel size,,,) \n",
        "\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.conv1t = nn.Conv1d(1,10, 30, padding ='same') #in_channels, out_channels, kernel_size, \n",
        "        self.conv1s = nn.Conv1d(10,10,self.channels)\n",
        "        self.conv2t = nn.Conv1d(10,20,15,padding ='same') \n",
        "        self.conv2s = nn.Conv1d(20,20,self.channels)\n",
        "        self.conv3t = nn.Conv1d(20,34,5,padding ='same')\n",
        "        self.conv3s = nn.Conv1d(34,34,self.channels)\n",
        "        \n",
        "                  \n",
        "        # Flatteninig\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Decision making\n",
        "        self.Linear = nn.Linear(256000 ,4) # \n",
        "\n",
        "      \n",
        "    def embedding(self, x):\n",
        "        x = self.activation(self.conv1t(x))\n",
        "        f1 = self.activation(self.conv1s(x))\n",
        "\n",
        "        x = self.activation(self.conv2t(x))\n",
        "        f2 = self.activation(self.conv2s(x))\n",
        "\n",
        "        x = self.activation(self.conv3t(x))\n",
        "        f3 = self.activation(self.conv3s(x))\n",
        "        \n",
        "        # multi-scale feature representation by exploiting intermediate features\n",
        "        feature = torch.cat([f1, f2, f3],dim = 1 )\n",
        "        \n",
        "        return feature\n",
        "\n",
        "    def classifier(self, feature):\n",
        "        # Flattening, dropout, mapping into the decision nodes\n",
        "        feature = self.flatten(feature)\n",
        "        feature = self.dropout(feature)\n",
        "        y_hat = self.softmax(self.Linear(feature))\n",
        "        return y_hat    \n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.embedding(x)\n",
        "        y_hat = self.classifier(feature)\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "17Wx4RicDlgZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialNetwork(nn.Module):\n",
        "    def __init__(self,temporal_len):\n",
        "        super(SpatialNetwork, self).__init__()\n",
        "\n",
        "        # input length/2 = output of conv1 length\n",
        "        # self.conv1 = nn.Conv1d(1, 16, stride = 2 ,padding = 14, kernel_size=30) \n",
        "        self.conv1 = nn.Conv1d(1, 16, padding = \"same\", kernel_size=30) \n",
        "        self.conv2 = nn.Conv1d(16, 32, padding=\"same\", kernel_size=15)\n",
        "        self.conv3 = nn.Conv1d(32, 64, padding=\"same\", kernel_size=5)\n",
        "\n",
        "\n",
        "        self.Spatial_conv_17 = nn.Conv2d(64,64,kernel_size = (17,1)) \n",
        "        self.maxpool = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # self.fc = nn.Linear(FL[-1] * 2, 5)\n",
        "        self.fc = nn.Linear(3069, 5)\n",
        "        self.elu = nn.ELU()\n",
        "        self.classifier = nn.Softmax(dim=1)\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # xavier initialization\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
        "\n",
        "    def embedding1(self, x, b, c):\n",
        "        # X=(B,C,T)\n",
        "\n",
        "        # X=(B*C,1,T)\n",
        "        x = self.elu(self.conv1(x))\n",
        "\n",
        "        # X=(B*C,F,T2)\n",
        "        x = self.elu(self.conv2(x))\n",
        "\n",
        "        # X=(B*C,F,T3)\n",
        "        x = self.elu(self.conv3(x))\n",
        "        \n",
        "        bc,f,t = x.shape \n",
        "        x = torch.reshape(x,(b,c,f,t))\n",
        "        \n",
        "        # X=(B,F,T4*2) # remove summation part, because each of dataset has a different number of channels\n",
        "        # x = torch.cat([x.max(dim=1)[0], x.sum(dim=1) / c], dim=1)       \n",
        "        return x\n",
        "\n",
        "    def embedding2(self, x, b, c):\n",
        "        x = self.embedding1(x,b,c)\n",
        "        b,c,f,t = x.shape\n",
        "        x = torch.reshape(x,(b,f,c,t))\n",
        "        if c == 17:\n",
        "          x = self.elu(self.Spatial_conv_17(x))\n",
        "          # print(x.shape) # b,64,1,t\n",
        "          # x = torch.squeeze(x)\n",
        "          # x = self.maxpool(x)\n",
        "             \n",
        "        return x    \n",
        "\n",
        "    def classifier(self, feature):\n",
        "        # Flattening, dropout, mapping into the decision nodes\n",
        "        # feature = torch.mean(feature, dim = 2) \n",
        "        feature = self.flatten(feature)\n",
        "        ## feature.shape = [batch,filter]\n",
        "\n",
        "        feature = self.dropout(feature)\n",
        "        y_hat = self.softmax(self.fc(feature))\n",
        "        return y_hat   \n",
        "    \n",
        "    def forward(self,x,b,c):\n",
        "        # print('1:',out.shape)\n",
        "        x = self.embedding2(x,b,c)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "suVNz0jDDljO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class StatisticianModule(nn.Module):\n",
        "    def __init__(self, dense, classes):\n",
        "        super(StatisticianModule, self).__init__()\n",
        "        self.classes = classes\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(64,64, (17,1))\n",
        "        self.conv2 = nn.Conv2d(64,64, (17,1))   \n",
        "        self.conv2 = nn.Conv2d(31,31, (17,1))\n",
        "\n",
        "\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.c_dense = nn.Linear(dense, 64*2*3) # 64*2*2\n",
        "\n",
        "        self.gap_pwconv = nn.Conv1d(192, dense, 1) \n",
        "        self.gvp_pwconv = nn.Conv1d(192, dense, 1)\n",
        "\n",
        "        self.fullconnect = nn.Linear(dense, self.classes)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.c_dense.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.gap_pwconv.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.gvp_pwconv.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.fullconnect.weight)\n",
        "        \n",
        "\n",
        "    def GAPGVP(self, x1, x2,x3, b,c,f,t ):\n",
        "        x1 = torch.reshape(x1,(b,f,c,t)) \n",
        "        x2 = torch.reshape(x2,(b,f,c,t)) \n",
        "        x3 = torch.reshape(x3,(b,f,c,t)) \n",
        "\n",
        "        x1 = self.conv1(x1)\n",
        "        x2 = self.conv2(x2)\n",
        "        x3 = self.conv3(x3)\n",
        "        # print('x1',x1.shape)\n",
        "        # print('x2',x2.shape)\n",
        "        # print('x3',x3.shape)\n",
        "\n",
        "        f_GAP = torch.cat((F.adaptive_avg_pool2d(x1, (1, 1)).squeeze(), F.adaptive_avg_pool2d(x2, (1, 1)).squeeze() ,F.adaptive_avg_pool2d(x3, (1, 1)).squeeze()), axis=1)\n",
        "        # print('f_GAP',f_GAP.shape)\n",
        "\n",
        "        f_GVP = torch.cat((torch.var(x1.view(x1.size(0), x1.size(1), -1), dim=2),torch.var(x2.view(x2.size(0), x2.size(1), -1), dim=2), torch.var(x3.view(x3.size(0), x3.size(1), -1), dim=2)),axis=1)\n",
        "        # print('f_GVP',f_GVP.shape)\n",
        "\n",
        "        del x1\n",
        "        del x2\n",
        "        del x3\n",
        "        return f_GAP, f_GVP\n",
        "\n",
        "    def forward(self,x1, x2,x3, b,c,f,t):\n",
        "        f_GAP, f_GVP = self.GAPGVP(x1, x2, x3, b,c,f,t )\n",
        "        # print(f_GAP.shape, f_GVP.shape)\n",
        "        c = self.softmax(self.c_dense(torch.cat((f_GAP, f_GVP),axis=1)))\n",
        "        # print('c:',c.shape)\n",
        "        #[batch, gap, 1] -> [batch, 1, dense] -> [batch, dense]\n",
        "        f_GAP_d = self.gap_pwconv(f_GAP.unsqueeze(dim=-1)).squeeze()\n",
        "        f_GVP_d = self.gvp_pwconv(f_GVP.unsqueeze(dim=-1)).squeeze()\n",
        "        \n",
        "        f_GAP_dd = torch.sum(c*f_GAP_d,dim=1)\n",
        "        f_GVP_dd = torch.sum(c*f_GVP_d,dim=1)\n",
        "        # print('f_GAP_dd: ',f_GAP_dd.shape)\n",
        "        ALN = torch.div(torch.sub(f_GAP_d.T,f_GAP_dd),f_GAP_dd).T\n",
        "        # print('ALN', ALN.shape)\n",
        "\n",
        "        y_hat = self.fullconnect(ALN)\n",
        "        # print('y_hat: ',y_hat.shape)\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "BFXKThj2OCHg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMqBq--jS9QX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load pretrain model"
      ],
      "metadata": {
        "id": "PK5lzwTNxWpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FE1 = torch.load('/content/Spectral__10s_ep6_.pt')\n",
        "FE2 = torch.load('/content/Spike_10s_ep8_.pt')\n",
        "FE3 = torch.load('/content/Temporal__10s_ep15_.pt')\n",
        "\n",
        "FE1.eval()\n",
        "FE2.eval()\n",
        "FE3.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYeo7tM_Gxsr",
        "outputId": "c4bb39f6-8674-4349-e997-9a461b67e1d5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "feature_extractor3(\n",
              "  (activation): LeakyReLU(negative_slope=0.01)\n",
              "  (softmax): Softmax(dim=None)\n",
              "  (conv1t): Conv1d(1, 10, kernel_size=(30,), stride=(1,), padding=same)\n",
              "  (conv1s): Conv1d(10, 10, kernel_size=(1,), stride=(1,))\n",
              "  (conv2t): Conv1d(10, 20, kernel_size=(15,), stride=(1,), padding=same)\n",
              "  (conv2s): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
              "  (conv3t): Conv1d(20, 34, kernel_size=(5,), stride=(1,), padding=same)\n",
              "  (conv3s): Conv1d(34, 34, kernel_size=(1,), stride=(1,))\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (Linear): Linear(in_features=128000, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#\"cpu\"\n",
        "\n",
        "print(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGeS3Hycs2gd",
        "outputId": "ca0ff29a-5a72-4b5c-abec-9846241b043e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "learning_rate = 0.0005\n",
        "batch_size = 10\n",
        "model = StatisticianModule(384,5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "1Sg0RcOsGxvT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "trainLoader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
        "valLoader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
        "testLoader = DataLoader(test_dataset, batch_size = batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "Mg_nmDT_iPbf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_tr = []\n",
        "loss_val = []\n",
        "acc_tr = []\n",
        "acc_val = []\n",
        "for epoch in range(epochs):\n",
        "    loss_ep = 0  # add batch loss in epoch\n",
        "    acc_ep = 0\n",
        "    for batch_idx, batch in enumerate(trainLoader):\n",
        "        optimizer.zero_grad()\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding1(data,b,c) \n",
        "        x3 =  FE3.embedding(data)\n",
        "        del data\n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,x3,b,c,f,t)\n",
        "        CrossEL = torch.nn.CrossEntropyLoss()\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "       \n",
        "        loss = CrossEL(pred, label)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step() \n",
        "\n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0] #acc/(batch*channels*4(augmented))\n",
        "        loss_ep += loss.item()\n",
        "        # print('acc:', acc)\n",
        "        acc_ep += acc\n",
        "\n",
        "    loss_tr.append((loss_ep)/len(trainLoader))\n",
        "    acc_tr.append((acc_ep)/len(trainLoader))        \n",
        "\n",
        "    loss_ep_val = 0\n",
        "    acc_ep_val = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(valLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding1(data,b,c) \n",
        "        x3 =  FE3.embedding(data)\n",
        "        del data\n",
        "\n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,x3,b,c,f,t)\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "\n",
        "       \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_val += loss.item()\n",
        "        acc_ep_val += acc\n",
        "\n",
        "    loss_val.append((loss_ep_val)/len(valLoader))\n",
        "    acc_val.append((acc_ep_val)/len(valLoader))\n",
        "    print(\"epoch : \", epoch, \"  train loss : \", loss_tr[epoch], 'train acc : ', acc_tr[epoch], \"    val loss : \", loss_val[epoch], 'val acc : ', acc_val[epoch])\n",
        "    torch.save(model,'MASS_10s_ep' + str(epoch)+'_.pt')\n"
      ],
      "metadata": {
        "id": "eEnhXWVI3so0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TxUJLBg3TfYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "col = ['loss_tr','loss_val','acc_tr','acc_val']\n",
        "data = np.array([loss_tr,\n",
        "                 loss_val,\n",
        "                 acc_tr,\n",
        "                 acc_val])\n",
        "print(data.shape)\n",
        "data = np.transpose(data)\n",
        "df = pd.DataFrame(data = data, columns= col)\n",
        "df.to_excel('MASS_10s.xlsx', index = False)"
      ],
      "metadata": {
        "id": "aHb22LwZ44el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 19"
      ],
      "metadata": {
        "id": "blKDYmwjt9p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "# plt.figure(figsize =(15, 10))\n",
        "plt.plot(range(epochs), loss_tr, color='red')\n",
        "plt.plot(range(epochs), loss_val, color='blue')\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.savefig('MASS_10s_loss.png',bbox_inches = 'tight')\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "K7x8bXNj38AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize =(15, 10))\n",
        "plt.plot(range(epochs), acc_tr, color='red')\n",
        "plt.plot(range(epochs), acc_val, color='blue')\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "plt.savefig('MASS_10s_accuracy.png',bbox_inches = 'tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xElqJIxCCz66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3Cjhmw_Cz9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/MASS_10s_ep15_.pt').to(device)"
      ],
      "metadata": {
        "id": "V4ccVb0ETOsY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_ep_test = 0\n",
        "acc_ep_test = 0\n",
        "for batch_idx, batch in enumerate(testLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding1(data,b,c) \n",
        "        x3 =  FE3.embedding(data)\n",
        "        del data\n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,x3,b,c,f,t)\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "        loss = CrossEL(pred, label)\n",
        "\n",
        "       \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        acc = (predicted == label).sum().item()\n",
        "        acc = acc/batch['x'].shape[0]\n",
        "\n",
        "        loss_ep_test += loss.item()\n",
        "        acc_ep_test += acc\n",
        "\n",
        "acc_ep_test = acc_ep_test/len(testLoader)\n",
        "loss_ep_test = loss_ep_test/len(testLoader)\n",
        "\n",
        "print(acc_ep_test)\n",
        "print(loss_ep_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GMhsbW-tHab",
        "outputId": "10f12f4b-0839-4e29-a5fc-7a5cf9eedbef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5682370820668693\n",
            "1.0883751597372946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = []\n",
        "predicted_list = []\n",
        "for batch_idx, batch in enumerate(testLoader):\n",
        "        b,c,t = batch['x'].shape\n",
        "        data = torch.reshape(batch['x'],(b*c,1,t))\n",
        "        data = torch.Tensor(data).type(torch.float).to(device)\n",
        "\n",
        "        x1 =  FE1.getRep(data)\n",
        "        x2 =  FE2.embedding1(data,b,c) \n",
        "        x3 =  FE3.embedding(data)\n",
        "        bc,f,t = x1.shape\n",
        "        pred  = model.forward(x1,x2,x3,b,c,f,t)\n",
        "        label = batch['y'].type(torch.float64).to(device)\n",
        "               \n",
        "        _, label =  torch.max(label, 1)  \n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        label_list.extend(label.cpu().detach().numpy())\n",
        "        predicted_list.extend(predicted.cpu().detach().numpy())"
      ],
      "metadata": {
        "id": "QAeKifEYQipU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix 사용을 위한 라이브러리\n",
        "from sklearn.metrics import confusion_matrix\n",
        "conf_matrix = confusion_matrix(label_list, predicted_list)\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d492032-cc7e-4490-c281-dbc1291ac9b5",
        "id": "4HlahGK0QipU"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[144   3  60   0  19]\n",
            " [ 51   6  69   0   7]\n",
            " [ 37   9 350   1   7]\n",
            " [  0   0  54   7   0]\n",
            " [ 24   3  61   0  25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "title = 'confusion matrix'\n",
        "cmap=plt.cm.Greens\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)  # , cmap=plt.cm.Greens\n",
        "plt.title(title, size=12)\n",
        "plt.colorbar(fraction=0.05, pad=0.05)\n",
        "tick_marks = np.arange(5, 5)\n",
        "plt.yticks(np.arange(5), ('W','N1','N2','N3','R'))\n",
        "plt.xticks(np.arange(5), ('W','N1','N2','N3','R'))\n",
        "\n",
        "\n",
        "fmt = 'd' \n",
        "thresh = 1\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, format(conf_matrix[i, j], fmt),\n",
        "                 ha=\"center\", va=\"center\", color=\"black\" if conf_matrix[i, j] > thresh else \"black\")  #horizontalalignment\n",
        "plt.savefig('MASS1_10s_confusion matrix.png',bbox_inches = 'tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "l1JMllMxSB7b",
        "outputId": "1666fd26-fb54-4232-f63e-9963ae4a92da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7GeRdvUDlmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXgSul4kDlpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c-y5mOdw49Y5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}